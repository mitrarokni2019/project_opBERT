{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-10T02:49:41.284358Z",
     "iopub.status.busy": "2024-12-10T02:49:41.283983Z",
     "iopub.status.idle": "2024-12-10T02:49:41.695863Z",
     "shell.execute_reply": "2024-12-10T02:49:41.694509Z",
     "shell.execute_reply.started": "2024-12-10T02:49:41.284321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/formerge/updated_data_new.csv\n",
      "/kaggle/input/formerge/labeled_sample_data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:49:42.449745Z",
     "iopub.status.busy": "2024-12-10T02:49:42.448481Z",
     "iopub.status.idle": "2024-12-10T02:49:46.344232Z",
     "shell.execute_reply": "2024-12-10T02:49:46.343206Z",
     "shell.execute_reply.started": "2024-12-10T02:49:42.449702Z"
    }
   },
   "outputs": [],
   "source": [
    "# merged labeled dataset to updata dataset > only last column:\n",
    "import pandas as pd \n",
    "# 1. Load dataset\n",
    "file_path_1 = \"/kaggle/input/formerge/labeled_sample_data.csv\"  # Update this\n",
    "data_1 = pd.read_csv(file_path_1)\n",
    "\n",
    "\n",
    "\n",
    "# 2. load \n",
    "file_path_2 = \"/kaggle/input/formerge/updated_data_new.csv\"  # Update this\n",
    "data_2 = pd.read_csv(file_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:49:59.590561Z",
     "iopub.status.busy": "2024-12-10T02:49:59.590141Z",
     "iopub.status.idle": "2024-12-10T02:49:59.598926Z",
     "shell.execute_reply": "2024-12-10T02:49:59.597655Z",
     "shell.execute_reply.started": "2024-12-10T02:49:59.590527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_id', 'link_id', 'subreddit_x', 'parent_id', 'redditor_id_x',\n",
       "       'created_at_x', 'body', 'score_x', 'edited_x', 'removed_x',\n",
       "       'submission_id', 'redditor_id_y', 'created_at_y', 'title', 'text',\n",
       "       'subreddit_y', 'permalink', 'attachment', 'flair', 'awards', 'score_y',\n",
       "       'upvote_ratio', 'num_comments', 'edited_y', 'archived', 'removed_y',\n",
       "       'poll', 'has_delta', 'cleaned_text', 'cleaned_body',\n",
       "       'is_climate_related', 'num_sentences_cleaned_body',\n",
       "       'num_sentences_cleaned_text', 'num_sentences_body',\n",
       "       'num_sentences_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:50:00.621154Z",
     "iopub.status.busy": "2024-12-10T02:50:00.620025Z",
     "iopub.status.idle": "2024-12-10T02:50:00.628670Z",
     "shell.execute_reply": "2024-12-10T02:50:00.627618Z",
     "shell.execute_reply.started": "2024-12-10T02:50:00.621101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_id', 'link_id', 'subreddit_x', 'parent_id', 'redditor_id_x',\n",
       "       'created_at_x', 'body', 'score_x', 'edited_x', 'removed_x',\n",
       "       'submission_id', 'redditor_id_y', 'created_at_y', 'title', 'text',\n",
       "       'subreddit_y', 'permalink', 'attachment', 'flair', 'awards', 'score_y',\n",
       "       'upvote_ratio', 'num_comments', 'edited_y', 'archived', 'removed_y',\n",
       "       'poll', 'has_delta', 'cleaned_text', 'cleaned_body',\n",
       "       'is_climate_related', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:50:27.982493Z",
     "iopub.status.busy": "2024-12-10T02:50:27.981508Z",
     "iopub.status.idle": "2024-12-10T02:50:28.388222Z",
     "shell.execute_reply": "2024-12-10T02:50:28.387122Z",
     "shell.execute_reply.started": "2024-12-10T02:50:27.982456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data neww saved to updated_data_with_sentences.csv\n",
      "Index(['comment_id', 'link_id', 'subreddit_x', 'parent_id', 'redditor_id_x',\n",
      "       'created_at_x', 'body', 'score_x', 'edited_x', 'removed_x',\n",
      "       'submission_id', 'redditor_id_y', 'created_at_y', 'title', 'text',\n",
      "       'subreddit_y', 'permalink', 'attachment', 'flair', 'awards', 'score_y',\n",
      "       'upvote_ratio', 'num_comments', 'edited_y', 'archived', 'removed_y',\n",
      "       'poll', 'has_delta', 'cleaned_text', 'cleaned_body',\n",
      "       'is_climate_related', 'label', 'num_sentences_cleaned_body',\n",
      "       'num_sentences_cleaned_text', 'num_sentences_body',\n",
      "       'num_sentences_text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Columns to add from data_2\n",
    "columns_to_add = [\n",
    "    'comment_id',  # Required for merging\n",
    "    'num_sentences_cleaned_body',\n",
    "    'num_sentences_cleaned_text',\n",
    "    'num_sentences_body',\n",
    "    'num_sentences_text'\n",
    "]\n",
    "\n",
    "# Select only the necessary columns from data_2\n",
    "data_2_subset = data_2[columns_to_add]\n",
    "\n",
    "# Merge data_1 with the selected columns from data_2 based on comment_id\n",
    "updated_data_neww = pd.merge(data_1, data_2_subset, on='comment_id', how='left')\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "output_path = \"updated_data_with_sentences.csv\"\n",
    "updated_data_neww.to_csv(output_path, index=False)\n",
    "print(f\"Updated data neww saved to {output_path}\")\n",
    "\n",
    "# Check the updated columns\n",
    "print(updated_data_neww.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:50:54.875488Z",
     "iopub.status.busy": "2024-12-10T02:50:54.874406Z",
     "iopub.status.idle": "2024-12-10T02:50:54.890805Z",
     "shell.execute_reply": "2024-12-10T02:50:54.889703Z",
     "shell.execute_reply.started": "2024-12-10T02:50:54.875448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 51209\n",
      "Unique comment_id values: 51209\n",
      "comment_id is unique in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# check if comment id is unoque \n",
    "\n",
    "# Check total number of rows in the dataset\n",
    "total_rows = len(data_2)\n",
    "\n",
    "# Check number of unique comment_id values\n",
    "unique_comment_ids = data_2['comment_id'].nunique()\n",
    "\n",
    "# Print results\n",
    "print(f\"Total rows in dataset: {total_rows}\")\n",
    "print(f\"Unique comment_id values: {unique_comment_ids}\")\n",
    "\n",
    "# Check if comment_id is unique\n",
    "if total_rows == unique_comment_ids:\n",
    "    print(\"comment_id is unique in the dataset.\")\n",
    "else:\n",
    "    print(\"comment_id is not unique in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:51:04.133979Z",
     "iopub.status.busy": "2024-12-10T02:51:04.133559Z",
     "iopub.status.idle": "2024-12-10T02:51:04.140814Z",
     "shell.execute_reply": "2024-12-10T02:51:04.139449Z",
     "shell.execute_reply.started": "2024-12-10T02:51:04.133944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blank or empty values in the 'label' column: 0\n"
     ]
    }
   ],
   "source": [
    "# Count blank or empty values in the 'label' column\n",
    "blank_or_empty_count = updated_data_neww['label'].isnull().sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of blank or empty values in the 'label' column: {blank_or_empty_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:51:06.548510Z",
     "iopub.status.busy": "2024-12-10T02:51:06.548162Z",
     "iopub.status.idle": "2024-12-10T02:51:06.556265Z",
     "shell.execute_reply": "2024-12-10T02:51:06.555137Z",
     "shell.execute_reply.started": "2024-12-10T02:51:06.548480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blank or empty values in the 'num_sentences_cleaned_body' column: 106\n",
      "Number of blank or empty values in the 'num_sentences_cleaned_text' column: 106\n",
      "Number of blank or empty values in the 'num_sentences_body' column: 106\n",
      "Number of blank or empty values in the 'num_sentences_text' column: 106\n"
     ]
    }
   ],
   "source": [
    "# List of new columns\n",
    "new_columns = ['num_sentences_cleaned_body', 'num_sentences_cleaned_text', 'num_sentences_body', 'num_sentences_text']\n",
    "\n",
    "# Check blank or empty values in each column\n",
    "for column in new_columns:\n",
    "    blank_or_empty_count = updated_data_neww[column].isnull().sum()\n",
    "    print(f\"Number of blank or empty values in the '{column}' column: {blank_or_empty_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:51:15.392792Z",
     "iopub.status.busy": "2024-12-10T02:51:15.392395Z",
     "iopub.status.idle": "2024-12-10T02:51:15.402340Z",
     "shell.execute_reply": "2024-12-10T02:51:15.401273Z",
     "shell.execute_reply.started": "2024-12-10T02:51:15.392757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null values: [10, 15, 37, 43, 47, 58, 71, 116, 278, 281, 304, 307, 323, 341, 349, 386, 387, 389, 390, 391, 413, 414, 418, 420, 450, 462, 470, 475, 477, 493, 545, 568, 569, 570, 597, 599, 612, 621, 632, 642, 643, 659, 665, 667, 668, 684, 688, 690, 691, 692, 704, 707, 852, 871, 876, 903, 920, 984, 1065, 1103, 1196, 1210, 1211, 1212, 1213, 1217, 1218, 1228, 1245, 1250, 1258, 1268, 1282, 1285, 1286, 1303, 1308, 1324, 1325, 1397, 1416, 1422, 1424, 1450, 1457, 1458, 1462, 1500, 1504, 1510, 1521, 1540, 1585, 1598, 1604, 1638, 1641, 1643, 1653, 1787, 1823, 1867, 1895, 1898, 1927, 1994]\n"
     ]
    }
   ],
   "source": [
    "# Identify rows with null or empty values in the specified columns\n",
    "rows_with_nulls_indices = updated_data_neww[\n",
    "    updated_data_neww[['num_sentences_cleaned_body', 'num_sentences_cleaned_text', 'num_sentences_body', 'num_sentences_text']].isnull().any(axis=1)\n",
    "].index\n",
    "\n",
    "# Display the row indices with nulls\n",
    "print(f\"Rows with null values: {list(rows_with_nulls_indices)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:53:33.913089Z",
     "iopub.status.busy": "2024-12-10T02:53:33.912294Z",
     "iopub.status.idle": "2024-12-10T02:53:34.110500Z",
     "shell.execute_reply": "2024-12-10T02:53:34.109439Z",
     "shell.execute_reply.started": "2024-12-10T02:53:33.913045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing values: 106\n",
      "Updated missing values per column:\n",
      "num_sentences_cleaned_body    0\n",
      "num_sentences_cleaned_text    0\n",
      "num_sentences_body            0\n",
      "num_sentences_text            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Function to count sentences in a text\n",
    "def count_sentences(text):\n",
    "    if pd.isna(text):  # Handle missing values\n",
    "        return 0\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "# Identify rows with null or empty values in the specified columns\n",
    "missing_rows = updated_data_neww[\n",
    "    updated_data_neww[['num_sentences_cleaned_body', 'num_sentences_cleaned_text', 'num_sentences_body', 'num_sentences_text']].isnull().any(axis=1)\n",
    "]\n",
    "\n",
    "print(f\"Rows with missing values: {len(missing_rows)}\")\n",
    "\n",
    "# Update the missing columns based on their corresponding text columns\n",
    "for idx in missing_rows.index:\n",
    "    updated_data_neww.at[idx, 'num_sentences_cleaned_body'] = count_sentences(updated_data_neww.loc[idx, 'cleaned_body'])\n",
    "    updated_data_neww.at[idx, 'num_sentences_cleaned_text'] = count_sentences(updated_data_neww.loc[idx, 'cleaned_text'])\n",
    "    updated_data_neww.at[idx, 'num_sentences_body'] = count_sentences(updated_data_neww.loc[idx, 'body'])\n",
    "    updated_data_neww.at[idx, 'num_sentences_text'] = count_sentences(updated_data_neww.loc[idx, 'text'])\n",
    "\n",
    "# Verify that there are no more missing values\n",
    "updated_missing_counts = updated_data_neww[['num_sentences_cleaned_body', 'num_sentences_cleaned_text', 'num_sentences_body', 'num_sentences_text']].isnull().sum()\n",
    "print(\"Updated missing values per column:\")\n",
    "print(updated_missing_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:54:42.635873Z",
     "iopub.status.busy": "2024-12-10T02:54:42.635450Z",
     "iopub.status.idle": "2024-12-10T02:54:42.643706Z",
     "shell.execute_reply": "2024-12-10T02:54:42.642555Z",
     "shell.execute_reply.started": "2024-12-10T02:54:42.635837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blank or empty values in the 'num_sentences_cleaned_body' column: 0\n",
      "Number of blank or empty values in the 'num_sentences_cleaned_text' column: 0\n",
      "Number of blank or empty values in the 'num_sentences_body' column: 0\n",
      "Number of blank or empty values in the 'num_sentences_text' column: 0\n"
     ]
    }
   ],
   "source": [
    "# List of new columns\n",
    "new_columns = ['num_sentences_cleaned_body', 'num_sentences_cleaned_text', 'num_sentences_body', 'num_sentences_text']\n",
    "\n",
    "# Check blank or empty values in each column\n",
    "for column in new_columns:\n",
    "    blank_or_empty_count = updated_data_neww[column].isnull().sum()\n",
    "    print(f\"Number of blank or empty values in the '{column}' column: {blank_or_empty_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:55:26.785291Z",
     "iopub.status.busy": "2024-12-10T02:55:26.784932Z",
     "iopub.status.idle": "2024-12-10T02:55:27.158757Z",
     "shell.execute_reply": "2024-12-10T02:55:27.157589Z",
     "shell.execute_reply.started": "2024-12-10T02:55:26.785260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to complete.csv. You can download it.\n"
     ]
    }
   ],
   "source": [
    "# Save the updated dataset to a CSV file\n",
    "output_path = \"complete.csv\"\n",
    "updated_data_neww.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Updated dataset saved to {output_path}. You can download it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:58:46.288626Z",
     "iopub.status.busy": "2024-12-10T02:58:46.288204Z",
     "iopub.status.idle": "2024-12-10T02:58:46.295928Z",
     "shell.execute_reply": "2024-12-10T02:58:46.294695Z",
     "shell.execute_reply.started": "2024-12-10T02:58:46.288567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_id', 'link_id', 'subreddit_x', 'parent_id', 'redditor_id_x',\n",
       "       'created_at_x', 'body', 'score_x', 'edited_x', 'removed_x',\n",
       "       'submission_id', 'redditor_id_y', 'created_at_y', 'title', 'text',\n",
       "       'subreddit_y', 'permalink', 'attachment', 'flair', 'awards', 'score_y',\n",
       "       'upvote_ratio', 'num_comments', 'edited_y', 'archived', 'removed_y',\n",
       "       'poll', 'has_delta', 'cleaned_text', 'cleaned_body',\n",
       "       'is_climate_related', 'label', 'num_sentences_cleaned_body',\n",
       "       'num_sentences_cleaned_text', 'num_sentences_body',\n",
       "       'num_sentences_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_data_neww.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6268634,
     "sourceId": 10153566,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
