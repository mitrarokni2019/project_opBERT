{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd4365e-7de0-4dce-873e-2e7b8bb2e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imposting libraries: \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93296c4-c685-4192-8cb1-6b644a9036a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../llm_opinion/newdatasets/datasetgroup/output/tendimension2_zeroshot.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loading dataset: \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#llm_opinion/newdatasets/datasetgroup/output/tendims_final2.csv\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#data= pd.read_csv(\"../llm_opinion/newdatasets/datasetgroup/output/tendims_final2.csv\") \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m data\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../llm_opinion/newdatasets/datasetgroup/output/tendimension2_zeroshot.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m      7\u001b[0m data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../llm_opinion/newdatasets/datasetgroup/output/tendimension2_zeroshot.csv'"
     ]
    }
   ],
   "source": [
    "# loading dataset: \n",
    "\n",
    "#llm_opinion/newdatasets/datasetgroup/output/tendims_final2.csv\n",
    "#data= pd.read_csv(\"../llm_opinion/newdatasets/datasetgroup/output/tendims_final2.csv\") \n",
    "data= pd.read_csv(\"./tendimension2_zeroshot.csv\") \n",
    "\n",
    "data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbc548-5f2b-4f9f-8b55-43fcf0141f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(\"\\nTotal number of comments:\", len(df))\n",
    "print(\"Number of delta comments:\", df['has_delta'].sum())\n",
    "print(\"Delta rate: {:.2f}%\".format((df['has_delta'].sum()/len(df))*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f93b5-4a6d-4b80-8de9-56456edd3a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dimension.values.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb75ae7-eafd-48b7-9a54-eb82e67d1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.semantic_dimension.values.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae059c-2cf6-47c7-b0a6-f1bd417ad659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_dimension_analysis(data):\n",
    "    dimensions = ['support', 'knowledge', 'conflict', 'power', \n",
    "                 'similarity', 'fun', 'status', 'trust', \n",
    "                 'identity', 'romance']\n",
    "    \n",
    "    results = []\n",
    "    for dim in dimensions:\n",
    "        dim_col = f'{dim}_mean'\n",
    "        delta_mean = data[data['has_delta'] == 1][dim_col].mean()\n",
    "        non_delta_mean = data[data['has_delta'] == 0][dim_col].mean()\n",
    "        \n",
    "        # Calculate odds ratio\n",
    "        median = data[dim_col].median()\n",
    "        contingency = pd.crosstab(data[dim_col] > median, data['has_delta'])\n",
    "        odds_ratio = (contingency.iloc[1,1] * contingency.iloc[0,0]) / \\\n",
    "                    (contingency.iloc[0,1] * contingency.iloc[1,0])\n",
    "        \n",
    "        results.append({\n",
    "            'dimension': dim,\n",
    "            'odds_ratio': odds_ratio,\n",
    "            'delta_mean': delta_mean,\n",
    "            'non_delta_mean': non_delta_mean\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot odds ratios\n",
    "    sns.barplot(data=results_df, x='dimension', y='odds_ratio')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Odds Ratios by Dimension')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run analysis and create plots\n",
    "results = plot_dimension_analysis(data)\n",
    "print(\"\\nResults sorted by odds ratio:\")\n",
    "print(results.sort_values('odds_ratio', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852c109-dc43-4c75-939e-b8cbb9462c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_dimensions(data):\n",
    "    dimensions = ['support', 'knowledge', 'conflict', 'power', \n",
    "                 'similarity', 'fun', 'status', 'trust', \n",
    "                 'identity', 'romance']\n",
    "    \n",
    "    results = []\n",
    "    for dim in dimensions:\n",
    "        dim_col = f'{dim}_mean'\n",
    "        \n",
    "        # Get means\n",
    "        delta_mean = data[data['has_delta'] == 1][dim_col].mean()\n",
    "        non_delta_mean = data[data['has_delta'] == 0][dim_col].mean()\n",
    "        \n",
    "        # Calculate odds ratio\n",
    "        median = data[dim_col].median()\n",
    "        high_score = data[dim_col] > median\n",
    "        contingency = pd.crosstab(high_score, data['has_delta'])\n",
    "        odds_ratio = (contingency.iloc[1,1] * contingency.iloc[0,0]) / \\\n",
    "                    (contingency.iloc[0,1] * contingency.iloc[1,0])\n",
    "        \n",
    "        results.append({\n",
    "            'dimension': dim,\n",
    "            'odds_ratio': odds_ratio,\n",
    "            'delta_mean': delta_mean,\n",
    "            'non_delta_mean': non_delta_mean\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results).sort_values('odds_ratio', ascending=False)\n",
    "    \n",
    "    # Create high-quality dot plot\n",
    "    plt.figure(figsize=(12, 8), dpi=300)\n",
    "    plt.scatter(results_df['odds_ratio'], \n",
    "               range(len(dimensions)),\n",
    "               color=['blue' if x > 1 else 'red' for x in results_df['odds_ratio']], \n",
    "               s=100)\n",
    "    \n",
    "    # Formatting\n",
    "    plt.yticks(range(len(dimensions)), results_df['dimension'], fontsize=12)\n",
    "    plt.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add values\n",
    "    for i, row in results_df.iterrows():\n",
    "        plt.text(row['odds_ratio'], i, f' {row[\"odds_ratio\"]:.2f}', \n",
    "                va='center', fontsize=12)\n",
    "    \n",
    "    plt.xlabel('Odds Ratio', fontsize=12)\n",
    "    plt.title('Social Dimensions Impact on Delta', fontsize=14, pad=20)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print formatted table\n",
    "    print(\"\\nResults sorted by odds ratio:\")\n",
    "    pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "    print(results_df[['dimension', 'odds_ratio', 'delta_mean', 'non_delta_mean']])\n",
    "    \n",
    "    return results_df, plt.gcf()\n",
    "\n",
    "# Run analysis\n",
    "results, fig = analyze_dimensions(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa632f-5143-4460-b0a7-8bbc43c21aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46463412-e79b-4a82-88b8-a4b1bc813fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_odds_ratios(data):\n",
    "    dimensions = ['knowledge', 'identity', 'conflict', 'power', \n",
    "                 'similarity', 'fun', 'status', 'trust', \n",
    "                 'support', 'romance']\n",
    "    \n",
    "    results = []\n",
    "    for dim in dimensions:\n",
    "        median = data[f'{dim}_mean'].median()\n",
    "        contingency = pd.crosstab(data[f'{dim}_mean'] > median, data['has_delta'])\n",
    "        \n",
    "        try:\n",
    "            odds_ratio = (contingency.iloc[1,1] * contingency.iloc[0,0]) / \\\n",
    "                        (contingency.iloc[0,1] * contingency.iloc[1,0])\n",
    "        except:\n",
    "            odds_ratio = np.nan\n",
    "            \n",
    "        results.append({\n",
    "            'dimension': dim,\n",
    "            'odds_ratio': odds_ratio\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results).sort_values('odds_ratio', ascending=False)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    bars = plt.barh(results_df['dimension'], results_df['odds_ratio'])\n",
    "    plt.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Odds Ratio')\n",
    "    plt.title('Impact of Dimensions on Delta Likelihood')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width, i, f' {width:.2f}', va='center')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "results = analyze_odds_ratios(data)\n",
    "print(\"\\nResults sorted by odds ratio (values > 1 indicate increased delta likelihood):\")\n",
    "print(results[['dimension', 'odds_ratio']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0a845-9232-4e5b-b9bb-004a81f49334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_social_dimensions_plot(results_df):\n",
    "    plt.figure(figsize=(8, 10), dpi=300)\n",
    "    \n",
    "    # Sort dimensions by odds ratio\n",
    "    results_df = results_df.sort_values('odds_ratio', ascending=True)\n",
    "    \n",
    "    # Create y-positions\n",
    "    y_pos = np.arange(len(results_df))\n",
    "    \n",
    "    # Create scatter plot\n",
    "    colors = ['red' if x <= 1 else 'blue' for x in results_df['odds_ratio']]\n",
    "    plt.scatter(results_df['odds_ratio'], y_pos, color=colors)\n",
    "    \n",
    "    # Add vertical line at x=1\n",
    "    plt.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add labels\n",
    "    plt.yticks(y_pos, results_df['dimension'])\n",
    "    \n",
    "    # Add odds ratio values\n",
    "    for i, v in enumerate(results_df['odds_ratio']):\n",
    "        plt.text(v, i, f' {v:.2f}', va='center')\n",
    "    \n",
    "    # Set axis limits and labels\n",
    "    plt.xlim(0, 2.5)\n",
    "    plt.xlabel('(a) Social dimensions')\n",
    "    \n",
    "    # Grid and formatting\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create plot\n",
    "create_social_dimensions_plot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5887b89b-f13c-4f68-bd7f-3b88c738cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_odds_ratios(data):\n",
    "    dimensions = ['support', 'knowledge', 'conflict', 'power', \n",
    "                 'similarity', 'fun', 'status', 'trust', \n",
    "                 'identity', 'romance']\n",
    "    \n",
    "    results = []\n",
    "    for dim in dimensions:\n",
    "        # For comments (cleaned_body)\n",
    "        body_median = data[f'{dim}_mean'].median()\n",
    "        body_contingency = pd.crosstab(\n",
    "            data['dimension'] == dim,\n",
    "            data['has_delta']\n",
    "        )\n",
    "        \n",
    "        # For posts (cleaned_text)\n",
    "        text_median = data[f'{dim}_mean_text'].median()\n",
    "        text_contingency = pd.crosstab(\n",
    "            data['dimension_text'] == dim,\n",
    "            data['has_delta']\n",
    "        )\n",
    "        \n",
    "        # Calculate odds ratios\n",
    "        try:\n",
    "            body_or = (body_contingency.iloc[1,1] * body_contingency.iloc[0,0]) / \\\n",
    "                     (body_contingency.iloc[0,1] * body_contingency.iloc[1,0])\n",
    "        except:\n",
    "            body_or = np.nan\n",
    "            \n",
    "        try:\n",
    "            text_or = (text_contingency.iloc[1,1] * text_contingency.iloc[0,0]) / \\\n",
    "                     (text_contingency.iloc[0,1] * text_contingency.iloc[1,0])\n",
    "        except:\n",
    "            text_or = np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'dimension': dim,\n",
    "            'comment_odds_ratio': body_or,\n",
    "            'post_odds_ratio': text_or\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Calculate and display results\n",
    "results = calculate_odds_ratios(data)\n",
    "print(\"\\nOdds Ratios (comparing delta vs non-delta):\")\n",
    "print(results.sort_values('comment_odds_ratio', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305db128-047f-4b60-b168-fc3f3fc8ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_odds_ratios(results_df):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), dpi=300)\n",
    "    \n",
    "    # Sort by odds ratios\n",
    "    results_df_body = results_df.sort_values('comment_odds_ratio', ascending=True)\n",
    "    results_df_text = results_df.sort_values('post_odds_ratio', ascending=True)\n",
    "    \n",
    "    # Comment plot\n",
    "    y_pos = np.arange(len(results_df))\n",
    "    colors1 = ['red' if x <= 1 else 'blue' for x in results_df_body['comment_odds_ratio']]\n",
    "    ax1.scatter(results_df_body['comment_odds_ratio'], y_pos, color=colors1)\n",
    "    ax1.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(results_df_body['semantic_dimension'])\n",
    "    \n",
    "    # Add values\n",
    "    for i, v in enumerate(results_df_body['comment_odds_ratio']):\n",
    "        if not np.isnan(v):\n",
    "            ax1.text(v, i, f' {v:.2f}', va='center')\n",
    "    \n",
    "    ax1.set_xlim(0, max(results_df_body['comment_odds_ratio'].max(), \n",
    "                        results_df_text['post_odds_ratio'].max()) * 1.2)\n",
    "    ax1.set_title('Comment (cleaned_body) Odds Ratios')\n",
    "    ax1.grid(False)\n",
    "    \n",
    "    # Post plot\n",
    "    y_pos = np.arange(len(results_df))\n",
    "    colors2 = ['red' if x <= 1 else 'blue' for x in results_df_text['post_odds_ratio']]\n",
    "    ax2.scatter(results_df_text['post_odds_ratio'], y_pos, color=colors2)\n",
    "    ax2.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(results_df_text['dimension'])\n",
    "    \n",
    "    # Add values\n",
    "    for i, v in enumerate(results_df_text['post_odds_ratio']):\n",
    "        if not np.isnan(v):\n",
    "            ax2.text(v, i, f' {v:.2f}', va='center')\n",
    "    \n",
    "    ax2.set_xlim(0, max(results_df_body['comment_odds_ratio'].max(), \n",
    "                        results_df_text['post_odds_ratio'].max()) * 1.2)\n",
    "    ax2.set_title('Post (cleaned_text) Odds Ratios')\n",
    "    ax2.grid(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create plots\n",
    "plot_odds_ratios(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa1305-87d4-4160-8cfe-a80291e8ce79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d685e0-0ef5-488c-90bb-e7df5473d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_odds_ratios(data):\n",
    "    dimensions = ['support', 'knowledge', 'conflict', 'power', \n",
    "                 'similarity', 'fun', 'status', 'trust', \n",
    "                 'identity', 'romance']\n",
    "    \n",
    "    results = []\n",
    "    for dim in dimensions:\n",
    "        # For comments (cleaned_body dimension)\n",
    "        dim_delta = len(data[(data['dimension'] == dim) & (data['has_delta'] == 1)])\n",
    "        dim_no_delta = len(data[(data['dimension'] == dim) & (data['has_delta'] == 0)])\n",
    "        other_delta = len(data[(data['dimension'] != dim) & (data['has_delta'] == 1)])\n",
    "        other_no_delta = len(data[(data['dimension'] != dim) & (data['has_delta'] == 0)])\n",
    "        \n",
    "        # For posts (dimension_text)\n",
    "        dim_text_delta = len(data[(data['dimension_text'] == dim) & (data['has_delta'] == 1)])\n",
    "        dim_text_no_delta = len(data[(data['dimension_text'] == dim) & (data['has_delta'] == 0)])\n",
    "        other_text_delta = len(data[(data['dimension_text'] != dim) & (data['has_delta'] == 1)])\n",
    "        other_text_no_delta = len(data[(data['dimension_text'] != dim) & (data['has_delta'] == 0)])\n",
    "        \n",
    "        # Calculate odds ratios\n",
    "        comment_or = ((dim_delta * other_no_delta) / \n",
    "                     (dim_no_delta * other_delta)) if (dim_no_delta * other_delta) != 0 else 0\n",
    "        \n",
    "        post_or = ((dim_text_delta * other_text_no_delta) / \n",
    "                  (dim_text_no_delta * other_text_delta)) if (dim_text_no_delta * other_text_delta) != 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'dimension': dim,\n",
    "            'comment_odds_ratio': comment_or,\n",
    "            'post_odds_ratio': post_or\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def plot_odds_ratios(results_df):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8), dpi=300)\n",
    "    \n",
    "    # Plot for comments\n",
    "    comment_data = results_df.sort_values('comment_odds_ratio', ascending=True)\n",
    "    y_pos = np.arange(len(comment_data))\n",
    "    colors = ['red' if x <= 1 else 'blue' for x in comment_data['comment_odds_ratio']]\n",
    "    \n",
    "    ax1.scatter(comment_data['comment_odds_ratio'], y_pos, color=colors)\n",
    "    ax1.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(comment_data['dimension'])\n",
    "    \n",
    "    for i, v in enumerate(comment_data['comment_odds_ratio']):\n",
    "        if v > 0:\n",
    "            ax1.text(v, i, f' {v:.2f}', va='center')\n",
    "    \n",
    "    ax1.set_title('Comment Odds Ratios')\n",
    "    ax1.set_xlabel('Odds Ratio')\n",
    "    \n",
    "    # Plot for posts\n",
    "    post_data = results_df.sort_values('post_odds_ratio', ascending=True)\n",
    "    y_pos = np.arange(len(post_data))\n",
    "    colors = ['red' if x <= 1 else 'blue' for x in post_data['post_odds_ratio']]\n",
    "    \n",
    "    ax2.scatter(post_data['post_odds_ratio'], y_pos, color=colors)\n",
    "    ax2.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(post_data['dimension'])\n",
    "    \n",
    "    for i, v in enumerate(post_data['post_odds_ratio']):\n",
    "        if v > 0:\n",
    "            ax2.text(v, i, f' {v:.2f}', va='center')\n",
    "    \n",
    "    ax2.set_title('Post Odds Ratios')\n",
    "    ax2.set_xlabel('Odds Ratio')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Calculate and visualize\n",
    "results = calculate_odds_ratios(data)\n",
    "fig = plot_odds_ratios(results)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOdds Ratios Results:\")\n",
    "print(results.sort_values('comment_odds_ratio', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03301d1-cf4a-455f-a856-3dee98b56c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dimension_counts(data):\n",
    "    dimensions = ['knowledge', 'identity', 'conflict', 'power', \n",
    "                 'similarity', 'fun', 'status', 'trust', \n",
    "                 'support', 'romance']\n",
    "    \n",
    "    print(\"Comment Dimension Counts:\")\n",
    "    print(\"-\" * 50)\n",
    "    for dim in dimensions:\n",
    "        delta = len(data[(data['dimension'] == dim) & (data['has_delta'] == 1)])\n",
    "        no_delta = len(data[(data['dimension'] == dim) & (data['has_delta'] == 0)])\n",
    "        total = len(data[data['dimension'] == dim])\n",
    "        print(f\"{dim}:\")\n",
    "        print(f\"Delta: {delta}, No Delta: {no_delta}, Total: {total}\")\n",
    "        print(f\"Delta Rate: {delta/total if total > 0 else 0:.4f}\")\n",
    "        print()\n",
    "\n",
    "verify_dimension_counts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe1f8e-bb79-40a2-b0d9-b27e80faaf81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16623c-13f3-4ce2-bc32-32b64bb8d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add confidence intervals\n",
    "results_df['error'] = np.sqrt(results_df['delta_rate'] * (1 - results_df['delta_rate']) / results_df['total_comments'])\n",
    "\n",
    "# Plot with error bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=results_df, x='delta_rate', y='dimension', ci=None)\n",
    "plt.errorbar(results_df['delta_rate'], results_df['dimension'], xerr=results_df['error'], fmt='none', c='black', capsize=5)\n",
    "plt.title('Delta Rate by Primary Dimension with Error Bars')\n",
    "plt.xlabel('Delta Rate')\n",
    "plt.ylabel('Primary Dimension')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54623b37-4b9e-4c4d-85be-342033875735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399df31-2f84-4adf-9eba-ce67058feaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_logistic_regression(data):\n",
    "    dimensions = ['support', 'knowledge', 'conflict', 'power', \n",
    "                 'similarity', 'fun', 'status', 'trust', \n",
    "                 'identity', 'romance']\n",
    "    \n",
    "    X = data[[f'{dim}_mean' for dim in dimensions]].fillna(0)\n",
    "    y = data['has_delta']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    coef_df = pd.DataFrame({\n",
    "        'dimension': dimensions,\n",
    "        'coefficient': model.coef_[0],\n",
    "        'exp_coef': np.exp(model.coef_[0])\n",
    "    })\n",
    "    coef_df['abs_coefficient'] = abs(coef_df['coefficient'])\n",
    "    coef_df = coef_df.sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['blue' if x > 0 else 'red' for x in coef_df['coefficient']]\n",
    "    plt.bar(coef_df['dimension'], coef_df['coefficient'], color=colors)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Dimension Coefficients in Predicting Delta')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(coef_df[['dimension', 'coefficient', 'exp_coef']].round(4))\n",
    "    \n",
    "    return model, coef_df\n",
    "\n",
    "model, coefficients = run_logistic_regression(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff3dde-cbb7-4f7b-81e0-ea893a74184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def enhanced_logistic_regression(data):\n",
    "    dimensions = ['support', 'knowledge', 'conflict', 'power', \n",
    "                 'similarity', 'fun', 'status', 'trust', \n",
    "                 'identity', 'romance']\n",
    "    \n",
    "    X = data[[f'{dim}_mean' for dim in dimensions]].fillna(0)\n",
    "    y = data['has_delta']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 1. ROC Curve\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(131)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve (AUC = {auc(fpr, tpr):.2f})')\n",
    "    \n",
    "    # 2. Feature Importance Plot\n",
    "    plt.subplot(132)\n",
    "    coef_df = pd.DataFrame({\n",
    "        'dimension': dimensions,\n",
    "        'coefficient': model.coef_[0],\n",
    "        'abs_coef': abs(model.coef_[0])\n",
    "    }).sort_values('abs_coef', ascending=True)\n",
    "    \n",
    "    colors = ['blue' if x > 0 else 'red' for x in coef_df['coefficient']]\n",
    "    plt.barh(coef_df['dimension'], coef_df['coefficient'], color=colors)\n",
    "    plt.title('Feature Coefficients')\n",
    "    \n",
    "    # 3. Confusion Matrix\n",
    "    plt.subplot(133)\n",
    "    cm = confusion_matrix(y_test, model.predict(X_test_scaled))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "    print(\"\\nCross-validation scores:\", cv_scores)\n",
    "    print(\"Mean CV score:\", cv_scores.mean())\n",
    "    \n",
    "    # 5. Probability Distribution\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    probas = model.predict_proba(X_test_scaled)[:,1]\n",
    "    plt.hist(probas[y_test==0], bins=50, alpha=0.5, label='Non-Delta', density=True)\n",
    "    plt.hist(probas[y_test==1], bins=50, alpha=0.5, label='Delta', density=True)\n",
    "    plt.xlabel('Predicted Probability of Delta')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Predicted Probabilities')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "enhanced_logistic_regression(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71292370-94ea-4417-a27c-db3be2aaa83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_logistic_regression(data):\n",
    "    dimensions = ['support', 'knowledge', 'conflict', 'power', \n",
    "                 'similarity', 'fun', 'status', 'trust', \n",
    "                 'identity', 'romance']\n",
    "    \n",
    "    X = data[[f'{dim}_mean' for dim in dimensions]].fillna(0)\n",
    "    y = data['has_delta']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    metrics = {\n",
    "        'Accuracy': (tp + tn) / (tp + tn + fp + fn),\n",
    "        'Precision': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
    "        'Recall': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "        'Specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        'F1-Score': 2 * (tp / (tp + fp)) * (tp / (tp + fn)) / ((tp / (tp + fp)) + (tp / (tp + fn))) if tp > 0 else 0\n",
    "    }\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "    ax1.set_title('Confusion Matrix')\n",
    "    ax1.set_ylabel('True Label')\n",
    "    ax1.set_xlabel('Predicted Label')\n",
    "    \n",
    "    # 2. Feature Importance\n",
    "    coef_df = pd.DataFrame({\n",
    "        'dimension': dimensions,\n",
    "        'coefficient': model.coef_[0]\n",
    "    }).sort_values('coefficient', ascending=True)\n",
    "    \n",
    "    colors = ['red' if x < 0 else 'blue' for x in coef_df['coefficient']]\n",
    "    ax2.barh(coef_df['dimension'], coef_df['coefficient'], color=colors)\n",
    "    ax2.set_title('Feature Coefficients')\n",
    "    \n",
    "    # 3. Probability Distribution\n",
    "    ax3.hist(y_pred_proba[y_test==0], bins=50, alpha=0.5, label='Non-Delta', density=True)\n",
    "    ax3.hist(y_pred_proba[y_test==1], bins=50, alpha=0.5, label='Delta', density=True)\n",
    "    ax3.set_xlabel('Predicted Probability')\n",
    "    ax3.set_ylabel('Density')\n",
    "    ax3.set_title('Prediction Distribution')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax4.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.2f})')\n",
    "    ax4.plot([0, 1], [0, 1], 'k--')\n",
    "    ax4.set_xlabel('False Positive Rate')\n",
    "    ax4.set_ylabel('True Positive Rate')\n",
    "    ax4.set_title('ROC Curve')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMetrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "# Run analysis\n",
    "model, metrics = analyze_logistic_regression(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ac8e1-dfce-4440-a42a-4ccc08b9756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def enhanced_logistic_regression(data):\n",
    "    dimensions = ['support', 'knowledge', 'conflict', 'power', \n",
    "                 'similarity', 'fun', 'status', 'trust', \n",
    "                 'identity', 'romance']\n",
    "    \n",
    "    X = data[[f'{dim}_mean' for dim in dimensions]].fillna(0)\n",
    "    y = data['has_delta']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 1. ROC Curve\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(131)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve (AUC = {auc(fpr, tpr):.2f})')\n",
    "    \n",
    "    # 2. Feature Importance Plot\n",
    "    plt.subplot(132)\n",
    "    coef_df = pd.DataFrame({\n",
    "        'dimension': dimensions,\n",
    "        'coefficient': model.coef_[0],\n",
    "        'abs_coef': abs(model.coef_[0])\n",
    "    }).sort_values('abs_coef', ascending=True)\n",
    "    \n",
    "    colors = ['blue' if x > 0 else 'red' for x in coef_df['coefficient']]\n",
    "    plt.barh(coef_df['dimension'], coef_df['coefficient'], color=colors)\n",
    "    plt.title('Feature Coefficients')\n",
    "    \n",
    "    # 3. Confusion Matrix\n",
    "    plt.subplot(133)\n",
    "    cm = confusion_matrix(y_test, model.predict(X_test_scaled))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "    print(\"\\nCross-validation scores:\", cv_scores)\n",
    "    print(\"Mean CV score:\", cv_scores.mean())\n",
    "    \n",
    "    # 5. Probability Distribution\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    probas = model.predict_proba(X_test_scaled)[:,1]\n",
    "    plt.hist(probas[y_test==0], bins=50, alpha=0.5, label='Non-Delta', density=True)\n",
    "    plt.hist(probas[y_test==1], bins=50, alpha=0.5, label='Delta', density=True)\n",
    "    plt.xlabel('Predicted Probability of Delta')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Predicted Probabilities')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "enhanced_logistic_regression(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d3794-ac6c-4705-9d79-e2b164c1a206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37af34-572e-483d-8b31-eb152df3d4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8bf92-16b1-4fb3-a1db-4a628fbb93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_semantic_dimensions(data):\n",
    "    # Calculate success rates by semantic dimension\n",
    "    semantic_stats = pd.DataFrame({\n",
    "        'total': data.groupby('semantic_dimension').size(),\n",
    "        'deltas': data[data['has_delta'] == 1].groupby('semantic_dimension').size()\n",
    "    }).fillna(0)\n",
    "    \n",
    "    semantic_stats['delta_rate'] = semantic_stats['deltas'] / semantic_stats['total']\n",
    "    semantic_stats = semantic_stats.sort_values('delta_rate', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Delta rates\n",
    "    semantic_stats['delta_rate'].plot(kind='bar', ax=ax1)\n",
    "    ax1.set_title('Delta Success Rate by Semantic Dimension')\n",
    "    ax1.set_ylabel('Delta Rate')\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Total counts\n",
    "    semantic_stats['total'].plot(kind='bar', ax=ax2)\n",
    "    ax2.set_title('Number of Comments by Semantic Dimension')\n",
    "    ax2.set_ylabel('Count')\n",
    "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return semantic_stats\n",
    "\n",
    "stats = analyze_semantic_dimensions(data)\n",
    "print(\"\\nDimension Statistics:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04599cf4-f877-4ba6-92a8-a67d095aadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_delta_success(data):\n",
    "    # Get only delta comments\n",
    "    delta_data = data[data['has_delta'] == 1]\n",
    "    \n",
    "    results = []\n",
    "    dimensions = data['semantic_dimension'].unique()\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        # All comments for this dimension\n",
    "        total = len(data[data['semantic_dimension'] == dim])\n",
    "        # Delta comments for this dimension\n",
    "        deltas = len(delta_data[delta_data['semantic_dimension'] == dim])\n",
    "        \n",
    "        # Calculate odds ratio\n",
    "        a = deltas  # delta & this dimension\n",
    "        b = total - deltas  # no delta & this dimension\n",
    "        c = len(delta_data) - deltas  # delta & other dimensions\n",
    "        d = len(data) - len(delta_data) - b  # no delta & other dimensions\n",
    "        \n",
    "        odds_ratio = (a * d) / (b * c) if (b * c) != 0 else np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'dimension': dim,\n",
    "            'total_comments': total,\n",
    "            'delta_comments': deltas,\n",
    "            'success_rate': deltas/total if total > 0 else 0,\n",
    "            'odds_ratio': odds_ratio\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('odds_ratio', ascending=False)\n",
    "\n",
    "results = analyze_delta_success(data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f18f03-2560-4e51-8ec3-46c751654c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_semantic_odds(data):\n",
    "    dimensions = data['semantic_dimension'].unique()\n",
    "    \n",
    "    results = []\n",
    "    for dim in dimensions:\n",
    "        # Count deltas and non-deltas for this dimension\n",
    "        dim_delta = len(data[(data['dimension'] == dim) & (data['has_delta'] == 1)])\n",
    "        dim_no_delta = len(data[(data['dimension'] == dim) & (data['has_delta'] == 0)])\n",
    "        \n",
    "        # Count for other dimensions\n",
    "        other_delta = len(data[(data['dimension'] != dim) & (data['has_delta'] == 1)])\n",
    "        other_no_delta = len(data[(data['dimension'] != dim) & (data['has_delta'] == 0)])\n",
    "        \n",
    "        # Calculate odds ratio\n",
    "        try:\n",
    "            odds_ratio = (dim_delta * other_no_delta) / (dim_no_delta * other_delta)\n",
    "        except:\n",
    "            odds_ratio = np.nan\n",
    "            \n",
    "        results.append({\n",
    "            'dimension': dim,\n",
    "            'odds_ratio': odds_ratio,\n",
    "            'total_comments': dim_delta + dim_no_delta,\n",
    "            'delta_comments': dim_delta,\n",
    "            'delta_rate': dim_delta/(dim_delta + dim_no_delta) if (dim_delta + dim_no_delta) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results).sort_values('odds_ratio', ascending=False)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), dpi=300)\n",
    "    \n",
    "    # Odds ratio plot\n",
    "    colors = ['blue' if x > 1 else 'red' for x in results_df['odds_ratio']]\n",
    "    bars = ax1.barh(results_df['dimension'], results_df['odds_ratio'], color=colors)\n",
    "    ax1.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.set_xlabel('Odds Ratio')\n",
    "    ax1.set_title('Impact of  Dimensions on Delta Likelihood')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        if not np.isnan(width):\n",
    "            ax1.text(width, i, f' {width:.2f}', va='center')\n",
    "    \n",
    "    # Comment counts plot\n",
    "    bars = ax2.barh(results_df['dimension'], results_df['total_comments'])\n",
    "    ax2.set_xlabel('Number of Comments')\n",
    "    ax2.set_title('Total Comments per Semantic Dimension')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width, i, f' {int(width)}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "results = analyze_semantic_odds(data)\n",
    "print(\"\\ngithub library embeddings Dimension Dimension Statistics:\")\n",
    "print(results[['dimension', 'odds_ratio', 'total_comments', 'delta_comments', 'delta_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f9ad2-ed85-4dea-a482-a05dec29443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_semantic_odds(data):\n",
    "    dimensions = data['semantic_dimension'].unique()\n",
    "    \n",
    "    results = []\n",
    "    for dim in dimensions:\n",
    "        # Count deltas and non-deltas for this dimension\n",
    "        dim_delta = len(data[(data['semantic_dimension'] == dim) & (data['has_delta'] == 1)])\n",
    "        dim_no_delta = len(data[(data['semantic_dimension'] == dim) & (data['has_delta'] == 0)])\n",
    "        \n",
    "        # Count for other dimensions\n",
    "        other_delta = len(data[(data['semantic_dimension'] != dim) & (data['has_delta'] == 1)])\n",
    "        other_no_delta = len(data[(data['semantic_dimension'] != dim) & (data['has_delta'] == 0)])\n",
    "        \n",
    "        # Calculate odds ratio\n",
    "        try:\n",
    "            odds_ratio = (dim_delta * other_no_delta) / (dim_no_delta * other_delta)\n",
    "        except:\n",
    "            odds_ratio = np.nan\n",
    "            \n",
    "        results.append({\n",
    "            'dimension': dim,\n",
    "            'odds_ratio': odds_ratio,\n",
    "            'total_comments': dim_delta + dim_no_delta,\n",
    "            'delta_comments': dim_delta,\n",
    "            'delta_rate': dim_delta/(dim_delta + dim_no_delta) if (dim_delta + dim_no_delta) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results).sort_values('odds_ratio', ascending=False)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), dpi=300)\n",
    "    \n",
    "    # Odds ratio plot\n",
    "    colors = ['blue' if x > 1 else 'red' for x in results_df['odds_ratio']]\n",
    "    bars = ax1.barh(results_df['dimension'], results_df['odds_ratio'], color=colors)\n",
    "    ax1.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.set_xlabel('Odds Ratio')\n",
    "    ax1.set_title('Impact of Semantic Dimensions on Delta Likelihood')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        if not np.isnan(width):\n",
    "            ax1.text(width, i, f' {width:.2f}', va='center')\n",
    "    \n",
    "    # Comment counts plot\n",
    "    bars = ax2.barh(results_df['dimension'], results_df['total_comments'])\n",
    "    ax2.set_xlabel('Number of Comments')\n",
    "    ax2.set_title('Total Comments per Semantic Dimension')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width, i, f' {int(width)}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "results = analyze_semantic_odds(data)\n",
    "print(\"\\nSemantic Dimension Statistics:\")\n",
    "print(results[['dimension', 'odds_ratio', 'total_comments', 'delta_comments', 'delta_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f7456-c6ce-462f-acf7-b14ab3edf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_method_comparison():\n",
    "    # Data\n",
    "    zeroshot_data = {\n",
    "        'trust': [1.553247, 331, 6, 0.018127],\n",
    "        'similarity': [1.473048, 14716, 221, 0.015018],\n",
    "        'power': [1.230356, 2732, 39, 0.014275],\n",
    "        'identity': [1.074614, 158, 2, 0.012658],\n",
    "        'support': [0.992591, 2647, 31, 0.011711],\n",
    "        'conflict': [0.908932, 17105, 190, 0.011108],\n",
    "        'knowledge': [0.858338, 2058, 21, 0.010204],\n",
    "        'fun': [0.499125, 1167, 7, 0.005998],\n",
    "        'status': [0.317925, 4443, 18, 0.004051],\n",
    "        'romance': [0.000000, 10, 0, 0.000000]\n",
    "    }\n",
    "    \n",
    "    embedding_data = {\n",
    "        'knowledge': [2.888002, 29171, 448, 0.015358],\n",
    "        'identity': [0.575262, 3955, 28, 0.007080],\n",
    "        'power': [0.451791, 922, 5, 0.005423],\n",
    "        'conflict': [0.426429, 8307, 47, 0.005658],\n",
    "        'similarity': [0.388069, 1069, 5, 0.004677],\n",
    "        'fun': [0.382972, 437, 2, 0.004577],\n",
    "        'support': [0.000000, 508, 0, 0.000000],\n",
    "        'status': [0.000000, 206, 0, 0.000000],\n",
    "        'trust': [0.000000, 732, 0, 0.000000],\n",
    "        'romance': [0.000000, 60, 0, 0.000000]\n",
    "    }\n",
    "    \n",
    "    dimensions = list(zeroshot_data.keys())\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Odds Ratio Comparison\n",
    "    zero_odds = [zeroshot_data[dim][0] for dim in dimensions]\n",
    "    emb_odds = [embedding_data[dim][0] for dim in dimensions]\n",
    "    \n",
    "    ax1.barh(dimensions, zero_odds, alpha=0.6, label='Zero-shot', color='blue')\n",
    "    ax1.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.set_title('Odds Ratios: Zero-shot')\n",
    "    \n",
    "    ax2.barh(dimensions, emb_odds, alpha=0.6, label='Embeddings', color='red')\n",
    "    ax2.axvline(x=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax2.set_title('Odds Ratios: Embeddings')\n",
    "    \n",
    "    # Success Rate Comparison\n",
    "    zero_rate = [zeroshot_data[dim][3] for dim in dimensions]\n",
    "    emb_rate = [embedding_data[dim][3] for dim in dimensions]\n",
    "    \n",
    "    ax3.barh(dimensions, zero_rate, alpha=0.6)\n",
    "    ax3.set_title('Success Rate: Zero-shot')\n",
    "    \n",
    "    ax4.barh(dimensions, emb_rate, alpha=0.6)\n",
    "    ax4.set_title('Success Rate: Embeddings')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print key differences\n",
    "    print(\"\\nKey differences between methods:\")\n",
    "    for dim in dimensions:\n",
    "        if abs(zeroshot_data[dim][0] - embedding_data[dim][0]) > 0.5:\n",
    "            print(f\"\\n{dim}:\")\n",
    "            print(f\"Zero-shot OR: {zeroshot_data[dim][0]:.3f}\")\n",
    "            print(f\"Embedding OR: {embedding_data[dim][0]:.3f}\")\n",
    "            print(f\"Difference: {abs(zeroshot_data[dim][0] - embedding_data[dim][0]):.3f}\")\n",
    "\n",
    "plot_method_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cce7db-9d52-48f9-988f-1c6e96b322e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_with_bert(data, batch_size=16):\n",
    "    # Prepare data\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(data['semantic_dimension'])\n",
    "    \n",
    "    # Tokenize texts\n",
    "    encodings = tokenizer(data['cleaned_body'].tolist(), \n",
    "                         truncation=True, \n",
    "                         padding=True, \n",
    "                         max_length=512,\n",
    "                         return_tensors='pt')\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = TensorDataset(\n",
    "        encodings['input_ids'],\n",
    "        encodings['attention_mask'],\n",
    "        torch.tensor(labels)\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, len(dataset) - train_size]\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=len(le.classes_)\n",
    "    )\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert predictions back to labels\n",
    "    pred_labels = le.inverse_transform(predictions)\n",
    "    true_labels = le.inverse_transform(true_labels)\n",
    "    \n",
    "    # Compare with other methods\n",
    "    comparison = pd.DataFrame({\n",
    "        'Text': data['cleaned_body'].iloc[len(dataset)-len(predictions):].reset_index(drop=True),\n",
    "        'BERT': pred_labels,\n",
    "        'Zero_shot': data['semantic_dimension'].iloc[len(dataset)-len(predictions):].reset_index(drop=True),\n",
    "        'Embedding': data['dimension'].iloc[len(dataset)-len(predictions):].reset_index(drop=True),\n",
    "        'Has_Delta': data['has_delta'].iloc[len(dataset)-len(predictions):].reset_index(drop=True)\n",
    "    })\n",
    "    \n",
    "    print(\"Classification Report (BERT):\")\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "    \n",
    "    return model, comparison\n",
    "\n",
    "# Run evaluation\n",
    "model, results = evaluate_with_bert(data)\n",
    "\n",
    "# Analyze agreement between methods\n",
    "agreement_stats = {\n",
    "    'BERT_vs_Zero': (results['BERT'] == results['Zero_shot']).mean(),\n",
    "    'BERT_vs_Embedding': (results['BERT'] == results['Embedding']).mean(),\n",
    "    'Zero_vs_Embedding': (results['Zero_shot'] == results['Embedding']).mean()\n",
    "}\n",
    "\n",
    "print(\"\\nAgreement between methods:\")\n",
    "for comparison, score in agreement_stats.items():\n",
    "    print(f\"{comparison}: {score:.3f}\")\n",
    "\n",
    "# Analyze success with deltas\n",
    "for method in ['BERT', 'Zero_shot', 'Embedding']:\n",
    "    success_rate = results[results['Has_Delta'] == 1][method].value_counts(normalize=True)\n",
    "    print(f\"\\nDelta success distribution for {method}:\")\n",
    "    print(success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ef714-e0e3-49e2-8f32-0f2f60025fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cop_impact(data):\n",
    "    # First, check data types and content\n",
    "    print(\"Checking data types:\")\n",
    "    print(data.dtypes)\n",
    "    \n",
    "    # Convert to datetime safely\n",
    "    try:\n",
    "        data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "        cop_date = pd.to_datetime('2023-12-13')\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting dates: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check data ranges\n",
    "    print(\"\\nDate range in data:\")\n",
    "    print(f\"Earliest: {data['created_at'].min()}\")\n",
    "    print(f\"Latest: {data['created_at'].max()}\")\n",
    "    \n",
    "    # Split data\n",
    "    before_cop = data[data['created_at'] < cop_date]\n",
    "    after_cop = data[data['created_at'] >= cop_date]\n",
    "    \n",
    "    print(f\"\\nComments before COP: {len(before_cop)}\")\n",
    "    print(f\"Comments after COP: {len(after_cop)}\")\n",
    "    \n",
    "    def get_dimension_stats(df, period):\n",
    "        if len(df) == 0:\n",
    "            print(f\"Warning: No data for period {period}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        dim_stats = []\n",
    "        for dim in df['dimension'].unique():\n",
    "            dim_data = df[df['dimension'] == dim]\n",
    "            stats = {\n",
    "                'period': period,\n",
    "                'dimension': dim,\n",
    "                'total_comments': len(dim_data),\n",
    "                'delta_comments': len(dim_data[dim_data['has_delta'] == 1]),\n",
    "                'delta_rate': len(dim_data[dim_data['has_delta'] == 1]) / len(dim_data) if len(dim_data) > 0 else 0\n",
    "            }\n",
    "            dim_stats.append(stats)\n",
    "        return pd.DataFrame(dim_stats)\n",
    "    \n",
    "    before_stats = get_dimension_stats(before_cop, 'Before COP28')\n",
    "    after_stats = get_dimension_stats(after_cop, 'After COP28')\n",
    "    \n",
    "    if len(before_stats) == 0 or len(after_stats) == 0:\n",
    "        print(\"Error: Insufficient data for comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure same dimensions in both periods\n",
    "    all_dims = sorted(list(set(before_stats['dimension'].unique()) | \n",
    "                         set(after_stats['dimension'].unique())))\n",
    "    \n",
    "    # Plot with error handling\n",
    "    try:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        x = np.arange(len(all_dims))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Ensure data exists for all dimensions\n",
    "        before_rates = [before_stats[before_stats['dimension'] == dim]['delta_rate'].iloc[0] \n",
    "                       if dim in before_stats['dimension'].values else 0 \n",
    "                       for dim in all_dims]\n",
    "        after_rates = [after_stats[after_stats['dimension'] == dim]['delta_rate'].iloc[0] \n",
    "                      if dim in after_stats['dimension'].values else 0 \n",
    "                      for dim in all_dims]\n",
    "        \n",
    "        ax1.bar(x - width/2, before_rates, width, label='Before COP28')\n",
    "        ax1.bar(x + width/2, after_rates, width, label='After COP28')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(all_dims, rotation=45)\n",
    "        ax1.set_title('Delta Success Rate by Period')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Same for comment counts\n",
    "        before_counts = [before_stats[before_stats['dimension'] == dim]['total_comments'].iloc[0] \n",
    "                        if dim in before_stats['dimension'].values else 0 \n",
    "                        for dim in all_dims]\n",
    "        after_counts = [after_stats[after_stats['dimension'] == dim]['total_comments'].iloc[0] \n",
    "                       if dim in after_stats['dimension'].values else 0 \n",
    "                       for dim in all_dims]\n",
    "        \n",
    "        ax2.bar(x - width/2, before_counts, width, label='Before COP28')\n",
    "        ax2.bar(x + width/2, after_counts, width, label='After COP28')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(all_dims, rotation=45)\n",
    "        ax2.set_title('Comment Distribution by Period')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plotting: {e}\")\n",
    "    \n",
    "    # Calculate changes\n",
    "    try:\n",
    "        changes = pd.merge(before_stats, after_stats, on='dimension', suffixes=('_before', '_after'))\n",
    "        changes['delta_rate_change'] = changes['delta_rate_after'] - changes['delta_rate_before']\n",
    "        changes['distribution_change'] = (changes['total_comments_after'] / after_stats['total_comments'].sum()) - \\\n",
    "                                       (changes['total_comments_before'] / before_stats['total_comments'].sum())\n",
    "        \n",
    "        return changes.sort_values('delta_rate_change', ascending=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating changes: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run analysis with error handling\n",
    "try:\n",
    "    results = analyze_cop_impact(data)\n",
    "    if results is not None:\n",
    "        print(\"\\nChanges in dimension effectiveness after COP28:\")\n",
    "        print(results[['dimension', 'delta_rate_change', 'distribution_change']])\n",
    "except Exception as e:\n",
    "    print(f\"Analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e30102-e6e6-4a0a-8f83-1fbb4eb6603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cop_impact(data):\n",
    "    # Convert to datetime and standardize timezone\n",
    "    data['created_at'] = pd.to_datetime(data['created_at']).dt.tz_localize(None)\n",
    "    cop_date = pd.to_datetime('2023-12-13')\n",
    "    \n",
    "    print(\"Date range in data:\")\n",
    "    print(f\"Earliest: {data['created_at'].min()}\")\n",
    "    print(f\"Latest: {data['created_at'].max()}\")\n",
    "    \n",
    "    # Split data\n",
    "    before_cop = data[data['created_at'] < cop_date]\n",
    "    after_cop = data[data['created_at'] >= cop_date]\n",
    "    \n",
    "    print(f\"\\nComments before COP: {len(before_cop)}\")\n",
    "    print(f\"Comments after COP: {len(after_cop)}\")\n",
    "    \n",
    "    def get_dimension_stats(df, period):\n",
    "        if len(df) == 0:\n",
    "            print(f\"Warning: No data for period {period}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        dim_stats = []\n",
    "        for dim in sorted(df['dimension'].unique()):\n",
    "            dim_data = df[df['dimension'] == dim]\n",
    "            stats = {\n",
    "                'period': period,\n",
    "                'dimension': dim,\n",
    "                'total_comments': len(dim_data),\n",
    "                'delta_comments': len(dim_data[dim_data['has_delta'] == 1]),\n",
    "                'delta_rate': len(dim_data[dim_data['has_delta'] == 1]) / len(dim_data) if len(dim_data) > 0 else 0\n",
    "            }\n",
    "            dim_stats.append(stats)\n",
    "        return pd.DataFrame(dim_stats)\n",
    "    \n",
    "    before_stats = get_dimension_stats(before_cop, 'Before COP28')\n",
    "    after_stats = get_dimension_stats(after_cop, 'After COP28')\n",
    "    \n",
    "    # Plot comparisons\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    dimensions = sorted(list(set(before_stats['dimension'].unique()) | \n",
    "                           set(after_stats['dimension'].unique())))\n",
    "    x = np.arange(len(dimensions))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Delta rates\n",
    "    before_rates = [before_stats[before_stats['dimension'] == dim]['delta_rate'].iloc[0] \n",
    "                   if dim in before_stats['dimension'].values else 0 \n",
    "                   for dim in dimensions]\n",
    "    after_rates = [after_stats[after_stats['dimension'] == dim]['delta_rate'].iloc[0] \n",
    "                  if dim in after_stats['dimension'].values else 0 \n",
    "                  for dim in dimensions]\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, before_rates, width, label='Before COP28')\n",
    "    bars2 = ax1.bar(x + width/2, after_rates, width, label='After COP28')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(dimensions, rotation=45, ha='right')\n",
    "    ax1.set_title('Delta Success Rate by Period')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}',\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    # Comment distribution\n",
    "    before_counts = [before_stats[before_stats['dimension'] == dim]['total_comments'].iloc[0] \n",
    "                    if dim in before_stats['dimension'].values else 0 \n",
    "                    for dim in dimensions]\n",
    "    after_counts = [after_stats[after_stats['dimension'] == dim]['total_comments'].iloc[0] \n",
    "                   if dim in after_stats['dimension'].values else 0 \n",
    "                   for dim in dimensions]\n",
    "    \n",
    "    bars3 = ax2.bar(x - width/2, before_counts, width, label='Before COP28')\n",
    "    bars4 = ax2.bar(x + width/2, after_counts, width, label='After COP28')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(dimensions, rotation=45, ha='right')\n",
    "    ax2.set_title('Comment Distribution by Period')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars3, bars4]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height)}',\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate changes\n",
    "    changes = pd.merge(before_stats, after_stats, on='dimension', suffixes=('_before', '_after'))\n",
    "    changes['delta_rate_change'] = changes['delta_rate_after'] - changes['delta_rate_before']\n",
    "    changes['distribution_change'] = (changes['total_comments_after'] / after_stats['total_comments'].sum()) - \\\n",
    "                                   (changes['total_comments_before'] / before_stats['total_comments'].sum())\n",
    "    \n",
    "    return changes.sort_values('delta_rate_change', ascending=False)\n",
    "\n",
    "# Run analysis\n",
    "results = analyze_cop_impact(data)\n",
    "print(\"\\nChanges in dimension effectiveness after COP28:\")\n",
    "print(results[['dimension', 'delta_rate_change', 'distribution_change']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a382c86-9a2b-4cf4-99dd-022597559c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def analyze_topics(data, n_topics=5):\n",
    "    # LDA Analysis\n",
    "    vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform(data['cleaned_body'])\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda_output = lda.fit_transform(doc_term_matrix)\n",
    "    \n",
    "    # Print top words per topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-10:-1]]\n",
    "        print(f\"\\nTopic {topic_idx + 1} top words:\")\n",
    "        print(\", \".join(top_words))\n",
    "    \n",
    "    # BERT embeddings\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def get_bert_embedding(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "    \n",
    "    # Get embeddings for a sample of comments\n",
    "    sample_size = min(1000, len(data))\n",
    "    sample_data = data.sample(n=sample_size, random_state=42)\n",
    "    embeddings = np.vstack([get_bert_embedding(text) for text in sample_data['cleaned_body']])\n",
    "    \n",
    "    # Cluster BERT embeddings\n",
    "    kmeans = KMeans(n_clusters=n_topics, random_state=42)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Analyze clusters\n",
    "    cluster_stats = []\n",
    "    for cluster in range(n_topics):\n",
    "        cluster_data = sample_data[clusters == cluster]\n",
    "        stats = {\n",
    "            'cluster': cluster,\n",
    "            'size': len(cluster_data),\n",
    "            'delta_rate': cluster_data['has_delta'].mean(),\n",
    "            'main_dimension': cluster_data['dimension'].mode().iloc[0]\n",
    "        }\n",
    "        cluster_stats.append(stats)\n",
    "    \n",
    "    return pd.DataFrame(cluster_stats)\n",
    "\n",
    "# Run analysis\n",
    "topic_results = analyze_topics(data)\n",
    "print(\"\\nCluster Statistics:\")\n",
    "print(topic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e45b6-74fa-43b1-bba0-7458d3379ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "def advanced_topic_analysis(data, n_topics=5):\n",
    "    # 1. Improved LDA Analysis\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=2000, \n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2)  # Include bigrams\n",
    "    )\n",
    "    doc_term_matrix = vectorizer.fit_transform(data['cleaned_body'])\n",
    "    \n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        learning_method='batch'\n",
    "    )\n",
    "    \n",
    "    # Get document-topic distributions\n",
    "    doc_topics = lda.fit_transform(doc_term_matrix)\n",
    "    \n",
    "    # Store results\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics_data = []\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        # Get top words with their weights\n",
    "        top_words_idx = topic.argsort()[:-20:-1]\n",
    "        topic_words = [(feature_names[i], topic[i]) for i in top_words_idx]\n",
    "        \n",
    "        # Get documents strongly associated with this topic\n",
    "        topic_docs = data[doc_topics[:, topic_idx] > np.percentile(doc_topics[:, topic_idx], 75)]\n",
    "        \n",
    "        topics_data.append({\n",
    "            'topic_id': topic_idx,\n",
    "            'top_words': topic_words,\n",
    "            'n_documents': len(topic_docs),\n",
    "            'delta_rate': topic_docs['has_delta'].mean(),\n",
    "            'dominant_dimension': topic_docs['dimension'].mode().iloc[0],\n",
    "            'dimension_distribution': topic_docs['dimension'].value_counts(normalize=True)\n",
    "        })\n",
    "    \n",
    "    # Visualizations\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Topic Word Importance\n",
    "    ax1 = plt.subplot(221)\n",
    "    for topic in topics_data:\n",
    "        word_weights = [w[1] for w in topic['top_words'][:10]]\n",
    "        ax1.plot(range(10), word_weights, marker='o', label=f'Topic {topic[\"topic_id\"]}')\n",
    "    ax1.set_xlabel('Top Words (by rank)')\n",
    "    ax1.set_ylabel('Word Importance')\n",
    "    ax1.set_title('Word Weight Distribution per Topic')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Delta Success by Topic\n",
    "    ax2 = plt.subplot(222)\n",
    "    delta_rates = [t['delta_rate'] for t in topics_data]\n",
    "    sns.barplot(x=range(n_topics), y=delta_rates, ax=ax2)\n",
    "    ax2.set_xlabel('Topic')\n",
    "    ax2.set_ylabel('Delta Rate')\n",
    "    ax2.set_title('Delta Success Rate by Topic')\n",
    "    \n",
    "    # 3. Topic-Dimension Heatmap\n",
    "    ax3 = plt.subplot(212)\n",
    "    dimension_matrix = pd.DataFrame([t['dimension_distribution'] for t in topics_data])\n",
    "    sns.heatmap(dimension_matrix, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax3)\n",
    "    ax3.set_title('Dimension Distribution across Topics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed topic information\n",
    "    print(\"\\nDetailed Topic Analysis:\")\n",
    "    for topic in topics_data:\n",
    "        print(f\"\\nTopic {topic['topic_id'] + 1}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"Top 10 Words:\")\n",
    "        for word, weight in topic['top_words'][:10]:\n",
    "            print(f\"{word}: {weight:.4f}\")\n",
    "        print(f\"\\nNumber of Documents: {topic['n_documents']}\")\n",
    "        print(f\"Delta Rate: {topic['delta_rate']:.4f}\")\n",
    "        print(f\"Dominant Dimension: {topic['dominant_dimension']}\")\n",
    "        print(\"\\nDimension Distribution:\")\n",
    "        for dim, prop in topic['dimension_distribution'].items():\n",
    "            print(f\"{dim}: {prop:.3f}\")\n",
    "    \n",
    "    return topics_data\n",
    "\n",
    "# Run analysis\n",
    "topics_data = advanced_topic_analysis(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7908e8-e1ab-4d9b-b620-94b903d211d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
