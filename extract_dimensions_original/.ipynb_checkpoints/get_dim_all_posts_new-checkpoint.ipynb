{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'thinc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtendims\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32m~\\op groups\\pds_opinions-main\\pds_opinions-main\\extract_dimensions\\tendims.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TweetTokenizer\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     11\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m tokenize \u001b[38;5;241m=\u001b[39m TweetTokenizer()\u001b[38;5;241m.\u001b[39mtokenize\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\compat.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'thinc'"
     ]
    }
   ],
   "source": [
    "import tendims\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mitra\\AppData\\Local\\Temp\\ipykernel_8436\\796914184.py\", line 1, in <module>\n",
      "    import gensim\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py\", line 11, in <module>\n",
      "    from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:F401\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\gensim\\parsing\\__init__.py\", line 4, in <module>\n",
      "    from .preprocessing import (  # noqa:F401\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\gensim\\parsing\\preprocessing.py\", line 26, in <module>\n",
      "    from gensim import utils\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\gensim\\utils.py\", line 35, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"C:\\Users\\mitra\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGensim installed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\parsing\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     preprocess_documents,\n\u001b[0;32m      6\u001b[0m     preprocess_string,\n\u001b[0;32m      7\u001b[0m     read_file,\n\u001b[0;32m      8\u001b[0m     read_files,\n\u001b[0;32m      9\u001b[0m     remove_stopwords,\n\u001b[0;32m     10\u001b[0m     split_alphanum,\n\u001b[0;32m     11\u001b[0m     stem_text,\n\u001b[0;32m     12\u001b[0m     strip_multiple_whitespaces,\n\u001b[0;32m     13\u001b[0m     strip_non_alphanum,\n\u001b[0;32m     14\u001b[0m     strip_numeric,\n\u001b[0;32m     15\u001b[0m     strip_punctuation,\n\u001b[0;32m     16\u001b[0m     strip_short,\n\u001b[0;32m     17\u001b[0m     strip_tags,\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\parsing\\preprocessing.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[0;32m     30\u001b[0m STOPWORDS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mless\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mover\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmove\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manyway\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthrough\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfifty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmill\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msomewhere\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monce\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     59\u001b[0m ])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\utils.py:35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmart_open\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m gensim_version\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py:295\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[0;32m     12\u001b[0m                            get_csr_submatrix)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compressed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(\"Gensim installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tendims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m tendims\u001b[38;5;241m.\u001b[39mTenDimensionsClassifier(is_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, models_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/lstm_trained_models\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      2\u001b[0m                                         embeddings_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdimensions_list\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(dimensions)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tendims' is not defined"
     ]
    }
   ],
   "source": [
    "model = tendims.TenDimensionsClassifier(is_cuda=False, models_dir = './models/lstm_trained_models', \n",
    "                                        embeddings_dir='./embeddings')\n",
    "dimensions = model.dimensions_list\n",
    "print(dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence-level classification\n",
    "\n",
    "The classifier was trained on individual sentences. Although the classifier accepts text of any length, we recommend to compute the scores sentence-by-sentence. The function compute_score_split does that for you and returns the maximum and average values. When using the maximum, please consider that the longer the text, the higher the likelihood to get a larger maximum value. So, if you use the maximum, be sure to account for text length in you analysis (i.e. a high maximum score on a text of 10 words is not comparable with the same value on a text of 100 words). You can always split the sentences yourself and aggregate sentence-level values as you deem appropriate.\n",
    "\n",
    "Score distribution\n",
    "\n",
    "The classifier returns confidence scores in the range [0,1]. This number is proportional to the likelihood of the text containing the selected dimension. Depending on the input data and on the aggregation performed, the empirical distributions of the confidence score may differ across dimensions (may be bell-shaped, skewed, bi-modal, etc.). For this reason, binarizing the scores based on a fixed threshold might not be the best approach. An approch that proved effective is to binarize based on a high percentile (e.g., 75th or 85th percentiles) computed on your empirical distribution of scores.\n",
    "\n",
    "Directionality\n",
    "\n",
    "The classifier was trained to identify expressions that \"convey\" dimension D from the speaker to the listener. For example, in the case of the dimension support, the classifier is supposed to find expressions indicating that the speaker is offering some support to the lister. In practice, this directionality is not guaranteed, and the classifier picks up different types of verbal expressions of the social dimensions. For example, \"I am willing to help you, whatever you need\" and \"Clara is willing to help George, whatever he needs\" have both relatively high scores for the dimension support (0.86 and 0.75, respectively), but only the first one is an expression of the speaker offering support. To more strongly enforce directionality, and approach that proved effective is to consider only sentences containing second-person pronouns.\n",
    "\n",
    "Errors\n",
    "\n",
    "Be aware that the classifier was trained mostly on Reddit data. It can be used on any piece of text but you should expect some performance drop when used on textual data with very different style or distribution of words (e.g., Twitter). Last, as everything in life, the classifications made by this tool are not perfect, but given eough data you'll be able to see interesting and meaningful trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "model.compute_score_split('Hello, my name is Mike. I am willing to help you, whatever you need.', dimensions='support')\n",
    "# (np.mean(scores), np.max(scores), np.min(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing individual threads and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_rknr7b = pd.read_csv(\"../data/updated_groupdata3_new.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_with_delta = thread_rknr7b[thread_rknr7b['has_delta'] == 1]\n",
    "posts_with_delta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment = posts_with_delta.iloc[0]['body']\n",
    "print(test_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Sentence Tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##In case you need to download:\n",
    "##!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(test_comment)\n",
    "\n",
    "# Extract sentences\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(test_comment)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'[^\\w\\s\\.\\?\\!]', '', text)  # Remove special characters except for ., ?, !\n",
    "    text = text.strip()  # Remove leading and trailing spaces\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('x200b', '') # Remove x200b\n",
    "    return text\n",
    "\n",
    "cleaned_test_comment = clean_text(test_comment)\n",
    "print(cleaned_test_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(cleaned_test_comment)\n",
    "\n",
    "# Extract sentences\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "thread_rknr7b['body_clean'] = thread_rknr7b[\"body\"].fillna(\"\").astype({\"body\":\"string\"}).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "model.compute_score_split(cleaned_test_comment, 'knowledge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results=[]\n",
    "for i in sentences:\n",
    "    results.append(model.compute_score_split(i, 'knowledge'))\n",
    "    print(f\"{i}:{model.compute_score_split(i, 'knowledge')}\")\n",
    "\n",
    "print(results)\n",
    "(np.mean(results), np.max(results), np.min(results), np.std(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_score_split(cleaned_test_comment, 'similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_score_split(cleaned_test_comment, 'trust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_comment=[]\n",
    "def compute_knowledge_scores(row):\n",
    "    try:\n",
    "        mean, max_score, min_score, std = model.compute_score_split(row['body_clean'], 'knowledge')\n",
    "        return pd.Series([mean, max_score, min_score, std], index=['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std'])\n",
    "    except ValueError as e:\n",
    "        problematic_comment.append({'post':row['link_id'],'index': row.name, 'comment': row['body_clean'], 'error': str(e)})\n",
    "        return pd.Series([None, None, None, None], index=['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std'])\n",
    "\n",
    "    \n",
    "thread_rknr7b[['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std']] = thread_rknr7b.apply(compute_knowledge_scores, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting 3 dimensions for all posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_posts = pd.read_csv(\"../data/posts_final.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text but keep sentence structure due to sentence-level classification\n",
    "def clean_text(row):\n",
    "    text = str(row[0])\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'[^\\w\\s\\.\\?\\!]', '', text)  # Remove special characters except for ., ?, !\n",
    "    text = text.strip()  # Remove leading and trailing spaces\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('x200b', '') # Remove x200b\n",
    "    return text\n",
    "\n",
    "cleaned_test_comment = clean_text(test_comment)\n",
    "print(cleaned_test_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_posts['body_clean'] = m_posts[[\"body\"]].astype({\"body\":\"string\"}).apply(clean_text,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_knowledge_scores(row):\n",
    "    try:\n",
    "        mean, max_score, min_score, std = model.compute_score_split(row['body_clean'], 'knowledge', min_tokens=3)\n",
    "        return pd.Series([mean, max_score, min_score, std], index=['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std'])\n",
    "    except ValueError as e:\n",
    "        problematic_comment.append({'post':row['post_id'],'index': row.name, 'comment': row['body_clean'], 'error': str(e)})\n",
    "        return pd.Series([None, None, None, None], index=['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std'])\n",
    "\n",
    "test_m_posts=m_posts[m_posts['link_id'].isin(['14z908l','17godlf'])]\n",
    "test_m_posts.loc[:,['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std']] = test_m_posts.apply(compute_knowledge_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_comment_pd=pd.DataFrame(problematic_comment)\n",
    "problematic_comment_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "nan_rows = test_m_posts[test_m_posts[['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std']].isna().any(axis=1)]\n",
    "#pd.DataFrame(nan_rows)\n",
    "pd.DataFrame(nan_rows.groupby(['body', 'body_clean']).size().reset_index(name='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nan_rows['body_clean'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_comment=[]\n",
    "def compute_similarity_scores(row):\n",
    "    try:\n",
    "        mean, max_score, min_score, std = model.compute_score_split(row['body_clean'], 'similarity')\n",
    "        return pd.Series([mean, max_score, min_score, std], index=['similarity_mean', 'similarity_max', 'similarity_min', 'similarity_std'])\n",
    "    except ValueError as e:\n",
    "        problematic_comment.append({'post':row['post_id'],'index': row.name, 'comment': row['body_clean'], 'error': str(e)})\n",
    "        return pd.Series([None, None, None, None], index=['similarity_mean', 'similarity_max', 'similarity_min', 'similarity_std'])\n",
    "\n",
    "test_m_posts[['similarity_mean', 'similarity_max', 'similarity_min', 'similarity_std']] = test_m_posts.apply(compute_similarity_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(problematic_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trust_scores(row):\n",
    "    try:\n",
    "        mean, max_score, min_score, std = model.compute_score_split(row['body_clean'], 'trust')\n",
    "        return pd.Series([mean, max_score, min_score, std], index=['trust_mean', 'trust_max', 'trust_min', 'trust_std'])\n",
    "    except ValueError as e:\n",
    "        problematic_comment.append({'post':row['post_id'],'index': row.name, 'comment': row['body_clean'], 'error': str(e)})\n",
    "        return pd.Series([None, None, None, None], index=['trust_mean', 'trust_max', 'trust_min', 'trust_std'])\n",
    "\n",
    "\n",
    "#test_m_posts[['trust_mean', 'trust_max', 'trust_min', 'trust_std']] = test_m_posts.apply(compute_trust_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_m_posts[['body','knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute for all posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_posts.loc[:,['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std']] = m_posts.apply(compute_knowledge_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_posts.loc[:,['similarity_mean', 'similarity_max', 'similarity_min', 'similarity_std']] = m_posts.apply(compute_similarity_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_posts.loc[:,['trust_mean', 'trust_max', 'trust_min', 'trust_std']] = m_posts.apply(compute_trust_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"../data/posts_with_dimesions.csv\"\n",
    "m_posts.to_csv(output_file, index=False)\n",
    "print(\"Saved file to:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # examples\n",
    "# sentences = {\n",
    "# 'knowledge' : [\n",
    "#     \"Only a fully trained Jedi Knight, with The Force as his ally, will conquer Vader and his Emperor. If you end your training now, if you choose the quick and easy path, as Vader did, you will become an agent of evil\",\n",
    "#     \"Well, in layman's terms, you use a rotating magnetic field to focus a narrow beam of gravitons; these in turn fold space-time consistent with Weyl tensor dynamics until the space-time curvature becomes infinitely large and you have a singularity\",\n",
    "#     \"Since positronic signatures have only been known to emanate from androids such as myself, it is logical to theorize that there is an android such as myself on Kolarus III\",\n",
    "# ],\n",
    "\n",
    "# 'power' : [\n",
    "#     \"Now if you don't want to be the fifth person ever to die in meta-shock from a planar rift, I suggest you get down behind that desk and don't move until we give you the signal\",\n",
    "#     \"You can ask any price you want, but you must give me those letters \",\n",
    "#     \"Right now you're in no position to ask questions! And your snide remarks...\"\n",
    "# ],\n",
    "\n",
    "# 'status' : [\n",
    "#     \"I want to thank you, sir, for giving me the opportunity to work\",\n",
    "#     \"Frankie, you're a good old man, and you've been loyal to my Father for years...so I hope you can explain what you mean\",\n",
    "#     \"And we drink to her, and we all congratulate her on her wonderful accomplishment during this last year...her great success in A Doll's House!\"\n",
    "# ],\n",
    "\n",
    "# 'trust' : [\n",
    "#     \"I'm trying to tell you – and this is where you have to trust me – but, I think your life might be in real danger\",\n",
    "#     \"Mr. Lebowski is prepared to make a generous offer to you to act as courier once we get instructions for the money\",\n",
    "#     \"Take the Holy Gospels in your hand and swear to tell the whole truth concerning everything you will be asked\"\n",
    "# ],\n",
    "\n",
    "# 'support' : [\n",
    "#     \"I'm sorry, I just feel like... I know I shouldn't ask, I just need some kind of help, I just, I have a deadline tomorrow\",\n",
    "#     \"Look, Dave, I know that you're sincere and that you're trying to do a competent job, and that you're trying to be helpful, but I can assure the problem is with the AO-units, and with your test gear\",\n",
    "#     \"Well... listen, if you need any help, you know, back up, call me, OK?\"\n",
    "# ],\n",
    "\n",
    "# 'romance' : [\n",
    "#     \"I'm going to marry the woman I love\",\n",
    "#     \"If you are truly wild at heart, you'll fight for your dreams... Don’t turn away from love, Sailor \",\n",
    "#     \"You admit to me you do not love your fiance?\"\n",
    "# ],\n",
    "\n",
    "# 'identity' : [\n",
    "#     \"Hey, I know what I'm talkin' about, black women ain't the same as white women \",\n",
    "#     \"That's how it was in the old world, Pop, but this is not Sicily\",\n",
    "#     \"But, as you are so fond of observing, Doctor, I'm not human\"\n",
    "# ],\n",
    "\n",
    "# 'fun' : [\n",
    "#     \"It’s just funny...who needs a serial psycho in the woods with a chainsaw when we have ourselves\",\n",
    "#     \"I do enjoy playing bingo, if you'd like to join me for a game tomorrow night at church you’re welcome to\",\n",
    "#     \"Oh, I'm sure it’s a lot of fun, 'cause the Incas did it, you know, and-and they-they-they were a million laughs\"\n",
    "# ],\n",
    "\n",
    "# 'conflict' : [\n",
    "#     \"Forgive me for askin', son, and I don’t mean to belabor the obvious, but why is it that you’ve got your head so far up your own ass?\",\n",
    "#     \"If you're lying to me you poor excuse for a human being, I'm gonna blow your brains all over this car\",\n",
    "#     \"I couldn't give a shit if you believe me or not, and frankly I'm too tired to prove it to you\"\n",
    "# ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for dim in sentences:\n",
    "#     print(f' === {dim.upper()} ===')\n",
    "#     for s in sentences[dim]:\n",
    "#         score = model.compute_score(s, dim)\n",
    "#         print (f'{s} -- {dim}={score:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
