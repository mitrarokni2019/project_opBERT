{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mitra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tendims\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(\"Gensim installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word embeddings from ./embeddings\\glove/glove.42B.300d.wv!\n",
      "Vocab size: 1917494\n",
      "Loaded word embeddings from ./embeddings\\word2vec/GoogleNews-vectors-negative300.wv!\n",
      "Vocab size: 3000000\n",
      "Loaded word embeddings from ./embeddings\\fasttext/wiki-news-300d-1M-subword.wv!\n",
      "Vocab size: 999994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mitra\\pds_opinions\\extract_dimensions\\tendims.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_state = torch.load(join(self.models_dir, modelname), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['support', 'knowledge', 'conflict', 'power', 'similarity', 'fun', 'status', 'trust', 'identity', 'romance']\n"
     ]
    }
   ],
   "source": [
    "model = tendims.TenDimensionsClassifier(is_cuda=False, models_dir = './models/lstm_trained_models', \n",
    "                                        embeddings_dir='./embeddings')\n",
    "dimensions = model.dimensions_list\n",
    "print(dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence-level classification\n",
    "\n",
    "The classifier was trained on individual sentences. Although the classifier accepts text of any length, we recommend to compute the scores sentence-by-sentence. The function compute_score_split does that for you and returns the maximum and average values. When using the maximum, please consider that the longer the text, the higher the likelihood to get a larger maximum value. So, if you use the maximum, be sure to account for text length in you analysis (i.e. a high maximum score on a text of 10 words is not comparable with the same value on a text of 100 words). You can always split the sentences yourself and aggregate sentence-level values as you deem appropriate.\n",
    "\n",
    "Score distribution\n",
    "\n",
    "The classifier returns confidence scores in the range [0,1]. This number is proportional to the likelihood of the text containing the selected dimension. Depending on the input data and on the aggregation performed, the empirical distributions of the confidence score may differ across dimensions (may be bell-shaped, skewed, bi-modal, etc.). For this reason, binarizing the scores based on a fixed threshold might not be the best approach. An approch that proved effective is to binarize based on a high percentile (e.g., 75th or 85th percentiles) computed on your empirical distribution of scores.\n",
    "\n",
    "Directionality\n",
    "\n",
    "The classifier was trained to identify expressions that \"convey\" dimension D from the speaker to the listener. For example, in the case of the dimension support, the classifier is supposed to find expressions indicating that the speaker is offering some support to the lister. In practice, this directionality is not guaranteed, and the classifier picks up different types of verbal expressions of the social dimensions. For example, \"I am willing to help you, whatever you need\" and \"Clara is willing to help George, whatever he needs\" have both relatively high scores for the dimension support (0.86 and 0.75, respectively), but only the first one is an expression of the speaker offering support. To more strongly enforce directionality, and approach that proved effective is to consider only sentences containing second-person pronouns.\n",
    "\n",
    "Errors\n",
    "\n",
    "Be aware that the classifier was trained mostly on Reddit data. It can be used on any piece of text but you should expect some performance drop when used on textual data with very different style or distribution of words (e.g., Twitter). Last, as everything in life, the classifications made by this tool are not perfect, but given eough data you'll be able to see interesting and meaningful trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7066471129655838,\n",
       " 0.9273629188537598,\n",
       " 0.48593130707740784,\n",
       " 0.22071580588817596)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "model.compute_score_split('Hello, my name is Mike. I am willing to help you, whatever you need.', dimensions='support')\n",
    "# (np.mean(scores), np.max(scores), np.min(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing individual threads and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'index', 'post_id', 'link_id', 'subreddit', 'parent_id',\n",
       "       'redditor_id', 'created_at', 'body', 'score', 'edited',\n",
       "       'parent_id_clean', 'has_delta', 'title', 'text', 'permalink',\n",
       "       'post_id_parent', 'redditor_id_parent', 'body_clean', 'knowledge_mean',\n",
       "       'knowledge_max', 'knowledge_min', 'knowledge_std', 'similarity_mean',\n",
       "       'similarity_max', 'similarity_min', 'similarity_std', 'trust_mean',\n",
       "       'trust_max', 'trust_min', 'trust_std', 'dominant_dimension_low_std',\n",
       "       'summarized_body_clean', 'cleaned_text', 'cleaned_body',\n",
       "       'is_climate_related', 'num_sentences_cleaned_body',\n",
       "       'num_sentences_cleaned_text', 'num_sentences_body',\n",
       "       'num_sentences_text', 'word_count_text', 'token_count_text',\n",
       "       'word_count_body', 'token_count_body', 'summarized_cleaned_bod',\n",
       "       'summarized_cleaned_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_data= pd.read_csv(\"../data/data_with_summarizes2.csv\") \n",
    "o_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['level_0', 'index', 'post_id', 'link_id', 'subreddit', 'parent_id',\n",
      "       'redditor_id', 'created_at', 'body', 'score', 'edited',\n",
      "       'parent_id_clean', 'has_delta', 'title', 'text', 'permalink',\n",
      "       'post_id_parent', 'redditor_id_parent', 'cleaned_text', 'cleaned_body',\n",
      "       'is_climate_related', 'num_sentences_cleaned_body',\n",
      "       'num_sentences_cleaned_text', 'num_sentences_body',\n",
      "       'num_sentences_text', 'word_count_text', 'token_count_text',\n",
      "       'word_count_body', 'token_count_body', 'summarized_cleaned_bod',\n",
      "       'summarized_cleaned_text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# List of columns to exclude\n",
    "\n",
    "columns_to_exclude = [\n",
    "    'body_clean', 'knowledge_mean', 'knowledge_max', 'knowledge_min',\n",
    "    'knowledge_std', 'similarity_mean', 'similarity_max', 'similarity_min',\n",
    "    'similarity_std', 'trust_mean', 'trust_max', 'trust_min', 'trust_std',\n",
    "    'dominant_dimension_low_std', 'summarized_body_clean'\n",
    "]\n",
    "\n",
    "# Create a new DataFrame without the excluded columns\n",
    "data = o_data.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Verify the resulting DataFrame columns\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>post_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>redditor_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>...</th>\n",
       "      <th>num_sentences_cleaned_body</th>\n",
       "      <th>num_sentences_cleaned_text</th>\n",
       "      <th>num_sentences_body</th>\n",
       "      <th>num_sentences_text</th>\n",
       "      <th>word_count_text</th>\n",
       "      <th>token_count_text</th>\n",
       "      <th>word_count_body</th>\n",
       "      <th>token_count_body</th>\n",
       "      <th>summarized_cleaned_bod</th>\n",
       "      <th>summarized_cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>219</td>\n",
       "      <td>19060</td>\n",
       "      <td>j5cvoh2</td>\n",
       "      <td>10i58lq</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>t1_j5cnfni</td>\n",
       "      <td>ttubtm05</td>\n",
       "      <td>2023-01-22 03:00:09+00:00</td>\n",
       "      <td>Sure like https://journals.plos.org/plosone/ar...</td>\n",
       "      <td>{'2024-10-29T11:53:29': 3}</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>299</td>\n",
       "      <td>347</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>sure like anyway the field is in its infancy b...</td>\n",
       "      <td>my view is basically this: 1. problems can be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>341</td>\n",
       "      <td>10909</td>\n",
       "      <td>j8g8has</td>\n",
       "      <td>111nzkp</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>t3_111nzkp</td>\n",
       "      <td>xttqr</td>\n",
       "      <td>2023-02-14 02:57:56+00:00</td>\n",
       "      <td>I think you're wrong about one crucial problem...</td>\n",
       "      <td>{'2024-10-29T08:33:27': 8}</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>863</td>\n",
       "      <td>932</td>\n",
       "      <td>259</td>\n",
       "      <td>259</td>\n",
       "      <td>i think you're wrong about one crucial problem...</td>\n",
       "      <td>irrational extremes. i feel like rational mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>423</td>\n",
       "      <td>10992</td>\n",
       "      <td>j8jfpjm</td>\n",
       "      <td>111nzkp</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>t1_j8iaui6</td>\n",
       "      <td>fqcei6ww</td>\n",
       "      <td>2023-02-14 20:24:24+00:00</td>\n",
       "      <td>&gt; Agreed, and this was pointed out in my origi...</td>\n",
       "      <td>{'2024-10-29T08:34:08': 2}</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>863</td>\n",
       "      <td>932</td>\n",
       "      <td>331</td>\n",
       "      <td>331</td>\n",
       "      <td>agreed, and this was pointed out in my origina...</td>\n",
       "      <td>irrational extremes. i feel like rational mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>481</td>\n",
       "      <td>5680</td>\n",
       "      <td>j8rqdnq</td>\n",
       "      <td>113q2vc</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>t3_113q2vc</td>\n",
       "      <td>n4ltg</td>\n",
       "      <td>2023-02-16 15:11:46+00:00</td>\n",
       "      <td>So, I agree with part of your conclusion. I do...</td>\n",
       "      <td>{'2024-10-29T06:56:25': 2}</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>992</td>\n",
       "      <td>1080</td>\n",
       "      <td>574</td>\n",
       "      <td>574</td>\n",
       "      <td>your conclusion. i agree with part of your con...</td>\n",
       "      <td>the usa consumes 10 times more than they are a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>483</td>\n",
       "      <td>5682</td>\n",
       "      <td>j8rzgra</td>\n",
       "      <td>113q2vc</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>t3_113q2vc</td>\n",
       "      <td>6vo8op</td>\n",
       "      <td>2023-02-16 16:16:18+00:00</td>\n",
       "      <td>I think it has less to do with a focus on the ...</td>\n",
       "      <td>{'2024-10-29T06:56:26': 1}</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>992</td>\n",
       "      <td>1080</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>i think it has less to do with a focus on the ...</td>\n",
       "      <td>the usa consumes 10 times more than they are a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     level_0  index  post_id  link_id     subreddit   parent_id redditor_id  \\\n",
       "219      219  19060  j5cvoh2  10i58lq  changemyview  t1_j5cnfni    ttubtm05   \n",
       "341      341  10909  j8g8has  111nzkp  changemyview  t3_111nzkp       xttqr   \n",
       "423      423  10992  j8jfpjm  111nzkp  changemyview  t1_j8iaui6    fqcei6ww   \n",
       "481      481   5680  j8rqdnq  113q2vc  changemyview  t3_113q2vc       n4ltg   \n",
       "483      483   5682  j8rzgra  113q2vc  changemyview  t3_113q2vc      6vo8op   \n",
       "\n",
       "                    created_at  \\\n",
       "219  2023-01-22 03:00:09+00:00   \n",
       "341  2023-02-14 02:57:56+00:00   \n",
       "423  2023-02-14 20:24:24+00:00   \n",
       "481  2023-02-16 15:11:46+00:00   \n",
       "483  2023-02-16 16:16:18+00:00   \n",
       "\n",
       "                                                  body  \\\n",
       "219  Sure like https://journals.plos.org/plosone/ar...   \n",
       "341  I think you're wrong about one crucial problem...   \n",
       "423  > Agreed, and this was pointed out in my origi...   \n",
       "481  So, I agree with part of your conclusion. I do...   \n",
       "483  I think it has less to do with a focus on the ...   \n",
       "\n",
       "                          score  ...  num_sentences_cleaned_body  \\\n",
       "219  {'2024-10-29T11:53:29': 3}  ...                           5   \n",
       "341  {'2024-10-29T08:33:27': 8}  ...                          12   \n",
       "423  {'2024-10-29T08:34:08': 2}  ...                          22   \n",
       "481  {'2024-10-29T06:56:25': 2}  ...                          26   \n",
       "483  {'2024-10-29T06:56:26': 1}  ...                           1   \n",
       "\n",
       "    num_sentences_cleaned_text  num_sentences_body num_sentences_text  \\\n",
       "219                          6                   5                 12   \n",
       "341                         28                  12                 32   \n",
       "423                         28                  22                 32   \n",
       "481                         40                  27                 48   \n",
       "483                         40                   1                 48   \n",
       "\n",
       "    word_count_text token_count_text word_count_body token_count_body  \\\n",
       "219             299              347             126              126   \n",
       "341             863              932             259              259   \n",
       "423             863              932             331              331   \n",
       "481             992             1080             574              574   \n",
       "483             992             1080              58               58   \n",
       "\n",
       "                                summarized_cleaned_bod  \\\n",
       "219  sure like anyway the field is in its infancy b...   \n",
       "341  i think you're wrong about one crucial problem...   \n",
       "423  agreed, and this was pointed out in my origina...   \n",
       "481  your conclusion. i agree with part of your con...   \n",
       "483  i think it has less to do with a focus on the ...   \n",
       "\n",
       "                               summarized_cleaned_text  \n",
       "219  my view is basically this: 1. problems can be ...  \n",
       "341  irrational extremes. i feel like rational mode...  \n",
       "423  irrational extremes. i feel like rational mode...  \n",
       "481  the usa consumes 10 times more than they are a...  \n",
       "483  the usa consumes 10 times more than they are a...  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_with_delta = data[data['has_delta'] == 1]\n",
    "posts_with_delta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure like https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126821\n",
      "\n",
      "Anyway the field is in its infancy but it would be startling if it weren't true, the strategy of \"more kids vs focus on providing more resources to fewer\" is genetic in every child reading species that we know about and one or the other is favored depending on the circumstances.  Humans don't seem to be an exception.\n",
      "\n",
      "So yeah, it's not like oxygen deprivation or something you can't evolve to change.  If we keep living in luxury we'll evolve to see population growth under luxury conditions.  The overall population may shrink before it grows, but some not yet fully identified subsets of people are growing today and will keep growing.\n"
     ]
    }
   ],
   "source": [
    "test_comment = posts_with_delta.iloc[0]['body']\n",
    "print(test_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sure like anyway the field is in its infancy but it would be startling if it weren't true, the strategy of more kids vs focus on providing more resources to fewer is genetic in every child reading species that we know about and one or the other is favored depending on the circumstances. humans don't seem to be an exception. so yeah, it's not like oxygen deprivation or something you can't evolve to change. if we keep living in luxury we'll evolve to see population growth under luxury conditions. the overall population may shrink before it grows, but some not yet fully identified subsets of people are growing today and will keep growing.\n"
     ]
    }
   ],
   "source": [
    "test_comment_1 = posts_with_delta.iloc[0]['cleaned_body']\n",
    "print(test_comment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sure like anyway the field is in its infancy but it would be startling if it weren't true, the strategy of more kids vs focus on providing more resources to fewer is genetic in every child reading species that we know about and one or the other is favored depending on the circumstances. humans don't seem to be an exception. so yeah, it's not like oxygen deprivation or something you can't evolve to change. if we keep living in luxury we'll evolve to see population growth under luxury conditions. the overall population may shrink before it grows, but some not yet fully identified subsets of people are growing today and will keep growing.\n"
     ]
    }
   ],
   "source": [
    "test_comment_2 = posts_with_delta.iloc[0]['summarized_cleaned_bod']\n",
    "print(test_comment_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Sentence Tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 45.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 40.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "##In case you need to download:\n",
    "##!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(test_comment_1)\n",
    "\n",
    "# Extract sentences\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(test_comment_1)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'[^\\w\\s\\.\\?\\!]', '', text)  # Remove special characters except for ., ?, !\n",
    "    text = text.strip()  # Remove leading and trailing spaces\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('x200b', '') # Remove x200b\n",
    "    return text\n",
    "\n",
    "cleaned_test_comment = preprocess(test_comment_1)\n",
    "print(cleaned_test_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(cleaned_test_comment)\n",
    "\n",
    "# Extract sentences\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "data['cleaned_body_v2'] = data[\"cleaned_body\"].fillna(\"\").astype({\"cleaned_body\":\"string\"}).apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_body_v2'] = data[\"cleaned_body\"].fillna(\"\").astype(str).apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "b= data['cleaned_body_v2'][3]\n",
    "model.compute_score_split(b, 'knowledge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_body_v2'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= data['cleaned_body'][3]\n",
    "print(c)\n",
    "\n",
    "model.compute_score_split(c, 'knowledge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= data['cleaned_body'][10]\n",
    "print(d)\n",
    "\n",
    "model.compute_score_split(d, 'knowledge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results=[]\n",
    "for i in sentences:\n",
    "    results.append(model.compute_score_split(i, 'knowledge'))\n",
    "    print(f\"{i}:{model.compute_score_split(i, 'knowledge')}\")\n",
    "print(\"---------------------\")\n",
    "print(results)\n",
    "(np.mean(results), np.max(results), np.min(results), np.std(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_score_split(data.cleaned_body[15], 'similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_score_split(data.cleaned_body[15], 'trust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_score_split(data.cleaned_body[15], 'knowledge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_score_split(data.cleaned_body[15], 'power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_comment=[]\n",
    "def compute_knowledge_scores(row):\n",
    "    try:\n",
    "        mean, max_score, min_score, std = model.compute_score_split(row['cleaned_body_v2'], 'knowledge')\n",
    "        return pd.Series([mean, max_score, min_score, std], index=['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std'])\n",
    "    except ValueError as e:\n",
    "        problematic_comment.append({'post':row['link_id'],'index': row.name, 'comment': row['body_clean'], 'error': str(e)})\n",
    "        return pd.Series([None, None, None, None], index=['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std'])\n",
    "\n",
    "    \n",
    "data[['knowledge_mean_10D', 'knowledge_max_10D', 'knowledge_min_10D', 'knowledge_std_10D']] = data.apply(compute_knowledge_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['cleaned_body_v2','knowledge_mean_10D', 'knowledge_max_10D', 'knowledge_min_10D', 'knowledge_std_10D']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_body_v2'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting 3 dimensions for all posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data= pd.read_csv(\"../data/data_with_summarizes2.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text but keep sentence structure due to sentence-level classification\n",
    "def clean_text(row):\n",
    "    text = str(row[0])\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'[^\\w\\s\\.\\?\\!]', '', text)  # Remove special characters except for ., ?, !\n",
    "    text = text.strip()  # Remove leading and trailing spaces\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('x200b', '') # Remove x200b\n",
    "    return text\n",
    "\n",
    "cleaned_test_comment = clean_text(test_comment_2)\n",
    "print(cleaned_test_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_body'] = data[[\"cleaned_body\"]].astype({\"cleaned_body\":\"string\"}).apply(cleaned_body,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_knowledge_scores(row):\n",
    "    try:\n",
    "        mean, max_score, min_score, std = model.compute_score_split(row['cleaned_body'], 'knowledge', min_tokens=3)\n",
    "        return pd.Series([mean, max_score, min_score, std], index=['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std'])\n",
    "    except ValueError as e:\n",
    "        problematic_comment.append({'post':row['post_id'],'index': row.name, 'comment': row['body_clean'], 'error': str(e)})\n",
    "        return pd.Series([None, None, None, None], index=['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std'])\n",
    "\n",
    "#test_m_posts=m_posts[m_posts['link_id'].isin(['14z908l','17godlf'])]\n",
    "data.loc[:,['knowledge_mean_10D_text', 'knowledge_max_10D', 'knowledge_min_10D', 'knowledge_std_10D']] = test_m_posts.apply(compute_knowledge_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_max_10D[['knowledge_mean_10D', 'knowledge_max1', 'knowledge_min_10D', 'knowledge_std_10D']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_comment_pd=pd.DataFrame(problematic_comment)\n",
    "problematic_comment_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "nan_rows = test_m_posts[test_m_posts[['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std']].isna().any(axis=1)]\n",
    "#pd.DataFrame(nan_rows)\n",
    "pd.DataFrame(nan_rows.groupby(['body', 'body_clean']).size().reset_index(name='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nan_rows['body_clean'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_comment=[]\n",
    "def compute_similarity_scores(row):\n",
    "    try:\n",
    "        mean, max_score, min_score, std = model.compute_score_split(row['body_clean'], 'similarity')\n",
    "        return pd.Series([mean, max_score, min_score, std], index=['similarity_mean', 'similarity_max', 'similarity_min', 'similarity_std'])\n",
    "    except ValueError as e:\n",
    "        problematic_comment.append({'post':row['post_id'],'index': row.name, 'comment': row['body_clean'], 'error': str(e)})\n",
    "        return pd.Series([None, None, None, None], index=['similarity_mean', 'similarity_max', 'similarity_min', 'similarity_std'])\n",
    "\n",
    "test_m_posts[['similarity_mean', 'similarity_max', 'similarity_min', 'similarity_std']] = test_m_posts.apply(compute_similarity_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(problematic_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trust_scores(row):\n",
    "    try:\n",
    "        mean, max_score, min_score, std = model.compute_score_split(row['body_clean'], 'trust')\n",
    "        return pd.Series([mean, max_score, min_score, std], index=['trust_mean', 'trust_max', 'trust_min', 'trust_std'])\n",
    "    except ValueError as e:\n",
    "        problematic_comment.append({'post':row['post_id'],'index': row.name, 'comment': row['body_clean'], 'error': str(e)})\n",
    "        return pd.Series([None, None, None, None], index=['trust_mean', 'trust_max', 'trust_min', 'trust_std'])\n",
    "\n",
    "\n",
    "#test_m_posts[['trust_mean', 'trust_max', 'trust_min', 'trust_std']] = test_m_posts.apply(compute_trust_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_m_posts[['body','knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute for all posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_posts.loc[:,['knowledge_mean', 'knowledge_max', 'knowledge_min', 'knowledge_std']] = m_posts.apply(compute_knowledge_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_posts.loc[:,['similarity_mean', 'similarity_max', 'similarity_min', 'similarity_std']] = m_posts.apply(compute_similarity_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_posts.loc[:,['trust_mean', 'trust_max', 'trust_min', 'trust_std']] = m_posts.apply(compute_trust_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"../data/posts_with_dimesions.csv\"\n",
    "m_posts.to_csv(output_file, index=False)\n",
    "print(\"Saved file to:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # examples\n",
    "# sentences = {\n",
    "# 'knowledge' : [\n",
    "#     \"Only a fully trained Jedi Knight, with The Force as his ally, will conquer Vader and his Emperor. If you end your training now, if you choose the quick and easy path, as Vader did, you will become an agent of evil\",\n",
    "#     \"Well, in layman's terms, you use a rotating magnetic field to focus a narrow beam of gravitons; these in turn fold space-time consistent with Weyl tensor dynamics until the space-time curvature becomes infinitely large and you have a singularity\",\n",
    "#     \"Since positronic signatures have only been known to emanate from androids such as myself, it is logical to theorize that there is an android such as myself on Kolarus III\",\n",
    "# ],\n",
    "\n",
    "# 'power' : [\n",
    "#     \"Now if you don't want to be the fifth person ever to die in meta-shock from a planar rift, I suggest you get down behind that desk and don't move until we give you the signal\",\n",
    "#     \"You can ask any price you want, but you must give me those letters \",\n",
    "#     \"Right now you're in no position to ask questions! And your snide remarks...\"\n",
    "# ],\n",
    "\n",
    "# 'status' : [\n",
    "#     \"I want to thank you, sir, for giving me the opportunity to work\",\n",
    "#     \"Frankie, you're a good old man, and you've been loyal to my Father for years...so I hope you can explain what you mean\",\n",
    "#     \"And we drink to her, and we all congratulate her on her wonderful accomplishment during this last year...her great success in A Doll's House!\"\n",
    "# ],\n",
    "\n",
    "# 'trust' : [\n",
    "#     \"I'm trying to tell you – and this is where you have to trust me – but, I think your life might be in real danger\",\n",
    "#     \"Mr. Lebowski is prepared to make a generous offer to you to act as courier once we get instructions for the money\",\n",
    "#     \"Take the Holy Gospels in your hand and swear to tell the whole truth concerning everything you will be asked\"\n",
    "# ],\n",
    "\n",
    "# 'support' : [\n",
    "#     \"I'm sorry, I just feel like... I know I shouldn't ask, I just need some kind of help, I just, I have a deadline tomorrow\",\n",
    "#     \"Look, Dave, I know that you're sincere and that you're trying to do a competent job, and that you're trying to be helpful, but I can assure the problem is with the AO-units, and with your test gear\",\n",
    "#     \"Well... listen, if you need any help, you know, back up, call me, OK?\"\n",
    "# ],\n",
    "\n",
    "# 'romance' : [\n",
    "#     \"I'm going to marry the woman I love\",\n",
    "#     \"If you are truly wild at heart, you'll fight for your dreams... Don’t turn away from love, Sailor \",\n",
    "#     \"You admit to me you do not love your fiance?\"\n",
    "# ],\n",
    "\n",
    "# 'identity' : [\n",
    "#     \"Hey, I know what I'm talkin' about, black women ain't the same as white women \",\n",
    "#     \"That's how it was in the old world, Pop, but this is not Sicily\",\n",
    "#     \"But, as you are so fond of observing, Doctor, I'm not human\"\n",
    "# ],\n",
    "\n",
    "# 'fun' : [\n",
    "#     \"It’s just funny...who needs a serial psycho in the woods with a chainsaw when we have ourselves\",\n",
    "#     \"I do enjoy playing bingo, if you'd like to join me for a game tomorrow night at church you’re welcome to\",\n",
    "#     \"Oh, I'm sure it’s a lot of fun, 'cause the Incas did it, you know, and-and they-they-they were a million laughs\"\n",
    "# ],\n",
    "\n",
    "# 'conflict' : [\n",
    "#     \"Forgive me for askin', son, and I don’t mean to belabor the obvious, but why is it that you’ve got your head so far up your own ass?\",\n",
    "#     \"If you're lying to me you poor excuse for a human being, I'm gonna blow your brains all over this car\",\n",
    "#     \"I couldn't give a shit if you believe me or not, and frankly I'm too tired to prove it to you\"\n",
    "# ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for dim in sentences:\n",
    "#     print(f' === {dim.upper()} ===')\n",
    "#     for s in sentences[dim]:\n",
    "#         score = model.compute_score(s, dim)\n",
    "#         print (f'{s} -- {dim}={score:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
