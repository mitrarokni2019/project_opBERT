{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b9b462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset saved to augmented_data2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def split_text(text, max_length=512):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_length):\n",
    "        chunks.append(' '.join(words[i:i + max_length]))\n",
    "    return chunks\n",
    "\n",
    "def back_translate(text, src_lang=\"en\", tgt_lang=\"fr\", max_length=512):\n",
    "    text_chunks = split_text(text, max_length)\n",
    "    back_translated_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        try:\n",
    "            model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
    "            tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "            model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "            translated = model.generate(**tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True))\n",
    "            translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "            model_name_back = f'Helsinki-NLP/opus-mt-{tgt_lang}-{src_lang}'\n",
    "            tokenizer_back = MarianTokenizer.from_pretrained(model_name_back)\n",
    "            model_back = MarianMTModel.from_pretrained(model_name_back)\n",
    "            back_translated = model_back.generate(**tokenizer_back(translated_text, return_tensors=\"pt\", padding=True, truncation=True))\n",
    "            back_translated_text = tokenizer_back.decode(back_translated[0], skip_special_tokens=True)\n",
    "\n",
    "            back_translated_chunks.append(back_translated_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Back translation failed for chunk: {e}\")\n",
    "            back_translated_chunks.append(chunk)\n",
    "\n",
    "    return ' '.join(back_translated_chunks)\n",
    "\n",
    "def random_word_order(text):\n",
    "    words = text.split()\n",
    "    random.shuffle(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def random_deletion(text, p=0.2):\n",
    "    words = text.split()\n",
    "    if len(words) == 1:  \n",
    "        return text\n",
    "    return ' '.join([word for word in words if random.random() > p])\n",
    "\n",
    "def random_insertion(text, additional_words=[\"cool\", \"awesome\", \"great\"], p=0.2):\n",
    "    words = text.split()\n",
    "    for _ in range(int(len(words) * p)):\n",
    "        index = random.randint(0, len(words))\n",
    "        word_to_add = random.choice(additional_words)\n",
    "        words.insert(index, word_to_add)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def augment_text(text):\n",
    "    augmented_texts = []\n",
    "\n",
    "    augmented_texts.append(back_translate(text))\n",
    "    augmented_texts.append(random_word_order(text))\n",
    "    augmented_texts.append(random_deletion(text))\n",
    "    augmented_texts.append(random_insertion(text))\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "def augment_dataset(file_path, output_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    augmented_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        post = row['cleaned_text']\n",
    "        comment = row['cleaned_body']\n",
    "        label = row['label_agreement']\n",
    "\n",
    "        augmented_posts = augment_text(post)\n",
    "        augmented_comments = augment_text(comment)\n",
    "\n",
    "        for aug_post, aug_comment in zip(augmented_posts, augmented_comments):\n",
    "            augmented_row = row.to_dict()  \n",
    "            augmented_row['cleaned_text'] = aug_post\n",
    "            augmented_row['cleaned_body'] = aug_comment\n",
    "            augmented_rows.append(augmented_row)\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "\n",
    "    full_df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "    full_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Augmented dataset saved to {output_path}\")\n",
    "\n",
    "input_csv = r'./labeled_sample_data.csv'\n",
    "output_csv = \"augmented_data2.csv\"  \n",
    "augment_dataset(input_csv, output_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479c368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
