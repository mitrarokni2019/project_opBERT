{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10127418,"sourceType":"datasetVersion","datasetId":6249766}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:03:27.762102Z","iopub.execute_input":"2024-12-09T21:03:27.762566Z","iopub.status.idle":"2024-12-09T21:03:28.251426Z","shell.execute_reply.started":"2024-12-09T21:03:27.762506Z","shell.execute_reply":"2024-12-09T21:03:28.249821Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mergeddataset/merged_data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import get_scheduler\nfrom tqdm import tqdm\n\n# Ensure GPU is enabled\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f\"Running on: {device}\")\n\n# 1. Load dataset\nfile_path = \"/kaggle/input/mergeddataset/merged_data.csv\"  # Update this\ndata = pd.read_csv(file_path)\n\n# Select a subset of 500 rows for testing\n#df = data[:500]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:03:28.366763Z","iopub.execute_input":"2024-12-09T21:03:28.367829Z","iopub.status.idle":"2024-12-09T21:03:38.973462Z","shell.execute_reply.started":"2024-12-09T21:03:28.367783Z","shell.execute_reply":"2024-12-09T21:03:38.972337Z"}},"outputs":[{"name":"stdout","text":"Running on: cpu\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# defining delta comments \n\nimport pandas as pd\nimport re\n\n# Define the function to check for delta\ndef check_for_delta(comment):\n    # Patterns to identify delta mentions\n    delta_patterns = [\n        r'!delta',                  # Direct delta command\n        r'delta\\s*awarded',         # Explicit delta award\n        r'Δ\\s*was\\s*awarded',       # Delta symbol with award language\n        r'changed\\s*my\\s*view',     # Alternative delta indication\n        r'you\\'?\\s*changed\\s*my\\s*view',  # Variations of view change\n        r'changed\\s*my\\s*perspective'     # Another view change variation\n    ]\n    \n    # Ensure comment is a string and do case-insensitive search\n    if not isinstance(comment, str):\n        return 0  # Return 0 for non-string comments\n    \n    return int(any(re.search(pattern, comment, re.IGNORECASE) for pattern in delta_patterns))\n\n# Apply the function to create the has_delta column\ndata['has_delta'] = data['body'].apply(check_for_delta)\n\n# Print delta statistics\ntotal_comments = len(data)\ndelta_comments = data[data['has_delta'] == 1]\nnum_delta_comments = len(delta_comments)\n\nprint(f\"Total comments: {total_comments}\")\nprint(f\"Comments with delta: {num_delta_comments}\")\nprint(f\"Percentage of delta comments: {num_delta_comments / total_comments * 100:.2f}%\")\n\n# Optional: Save the updated dataset\n# data.to_csv('/kaggle/input/mydata/merged_data_with_deltas.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:03:38.975728Z","iopub.execute_input":"2024-12-09T21:03:38.976140Z","iopub.status.idle":"2024-12-09T21:03:40.951035Z","shell.execute_reply.started":"2024-12-09T21:03:38.976101Z","shell.execute_reply":"2024-12-09T21:03:40.949593Z"}},"outputs":[{"name":"stdout","text":"Total comments: 53983\nComments with delta: 918\nPercentage of delta comments: 1.70%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"delta_comments['body'][1246]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:03:40.952916Z","iopub.execute_input":"2024-12-09T21:03:40.953511Z","iopub.status.idle":"2024-12-09T21:03:40.963638Z","shell.execute_reply.started":"2024-12-09T21:03:40.953455Z","shell.execute_reply":"2024-12-09T21:03:40.962310Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"\"!delta\\n\\nThis right here is the summary of what I'm seeing in the various parent groups, including the one and done group. Society is hell bent on not supporting pregnancy women, babies, parents, or families, and making the entire endeavor as unobtainable and expensive as possible.\""},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# checking for if post are duplicated \n\n# Count initial unique posts based on 'link_id'\ninitial_posts = data['link_id'].nunique()\n\n# Drop duplicate posts based on 'link_id' and keep the first occurrence\ndata_unique_posts = data.drop_duplicates(subset='link_id', keep='first')\n\n# Count the remaining unique posts\nremaining_posts = data_unique_posts['link_id'].nunique()\n\n# Calculate the number of duplicate posts removed\nduplicate_posts = initial_posts - remaining_posts\n\n# Display results\nprint(f\"Total initial posts: {initial_posts}\")\nprint(f\"Duplicate posts removed: {duplicate_posts}\")\nprint(f\"Remaining unique posts: {remaining_posts}\")\n\n# Save the cleaned dataset\ndata_unique_posts.to_csv('unique_posts_before_preprocessing.csv', index=False)\nprint(\"Dataset with unique posts saved as 'unique_posts_before_preprocessing.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:03:40.966270Z","iopub.execute_input":"2024-12-09T21:03:40.966710Z","iopub.status.idle":"2024-12-09T21:03:41.028927Z","shell.execute_reply.started":"2024-12-09T21:03:40.966669Z","shell.execute_reply":"2024-12-09T21:03:41.027793Z"}},"outputs":[{"name":"stdout","text":"Total initial posts: 331\nDuplicate posts removed: 0\nRemaining unique posts: 331\nDataset with unique posts saved as 'unique_posts_before_preprocessing.csv'\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# checking number of posts and number of commmetns :\n\n# Group by 'link_id' (post ID) and count the number of comments\npost_comment_counts = data.groupby('link_id').size().reset_index(name='num_comments')\n\n# Display the result\nprint(\"number of comments\",len(data))\nprint(\"number of posts\",len(post_comment_counts))\nprint(\"-------------------------------------------\")\nprint(post_comment_counts.head())\n\n#save dataset of posts and num of commetns \n#data.to_csv('/kaggle/input/mergeddataset/data1.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:03:41.030299Z","iopub.execute_input":"2024-12-09T21:03:41.030651Z","iopub.status.idle":"2024-12-09T21:03:41.052527Z","shell.execute_reply.started":"2024-12-09T21:03:41.030616Z","shell.execute_reply":"2024-12-09T21:03:41.051045Z"}},"outputs":[{"name":"stdout","text":"number of comments 53983\nnumber of posts 331\n-------------------------------------------\n   link_id  num_comments\n0  10a2an3           209\n1  10cktde            46\n2  10djn5r           179\n3  10i58lq           169\n4  10li7zz           138\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# this cleanning preprocessing is for bert \n\nimport pandas as pd\nimport re\n\n# Define an extended cleaning function\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Handle common encoding issues\n    text = text.replace(\"â€™\", \"'\").replace(\"â€œ\", '').replace(\"â€\", '')\n    text = text.replace(\"â€“\", \"-\").replace(\"â€¦\", \"...\").replace(\"â€\", \"\")\n    \n    # Remove URLs\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n    \n    # Remove HTML tags\n    text = re.sub(r\"<.*?>\", '', text)\n    \n    # Remove email addresses\n    text = re.sub(r'\\S+@\\S+', '', text)\n    \n    # Remove mentions (e.g., @username)\n    text = re.sub(r\"@\\w+\", '', text)\n    \n    # Remove special characters but retain mathematical symbols, percentages, and numbers\n    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?%'():;+-/]\", '', text)\n    \n    # Normalize multiple spaces\n    text = re.sub(r\"\\s+\", ' ', text).strip()\n\n    # Replace common escape sequences with their intended characters\n    text = text.replace(\"\\\\'\", \"'\")  # Fix escaped single quotes\n    text = text.replace('\\\\\"', '')  # Remove escaped double quotes\n    \n    # Normalize smart quotes (curly quotes) to standard quotes or remove them\n    text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\")  # Replace left and right single quotes\n    text = text.replace(\"“\", '').replace(\"”\", '')  # Remove left and right double quotes\n    \n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n    text = text.strip()  # Remove leading/trailing spaces  \n    \n    # Remove extra spaces around quotes\n    text = re.sub(r\"\\s+'\", \"'\", text)  # Remove space before single quotes\n    text = re.sub(r\"'\\s+\", \"'\", text)  # Remove space after single quotes\n    text = re.sub(r'\\s+\"', '', text)  # Remove space before double quotes\n    text = re.sub(r'\"\\s+', '', text)  # Remove space after double quotes\n   \n    return text\n\n# Apply cleaning to the `text` and `body` columns\ndata['cleaned_text'] = data['text'].apply(clean_text)\ndata['cleaned_body'] = data['body'].apply(clean_text)\n\n# Display a sample of the cleaned data\ndata[['text', 'cleaned_text', 'body', 'cleaned_body']].head()\n\n# Optional: Save the updated dataset\n#df.to_csv('/kaggle/input/mergeddataset/data1.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:03:45.801849Z","iopub.execute_input":"2024-12-09T21:03:45.803269Z","iopub.status.idle":"2024-12-09T21:04:26.105425Z","shell.execute_reply.started":"2024-12-09T21:03:45.803199Z","shell.execute_reply":"2024-12-09T21:04:26.104170Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  When you ask the average conservative why they...   \n1  When you ask the average conservative why they...   \n2  When you ask the average conservative why they...   \n3  When you ask the average conservative why they...   \n4  When you ask the average conservative why they...   \n\n                                        cleaned_text  \\\n0  when you ask the average conservative why they...   \n1  when you ask the average conservative why they...   \n2  when you ask the average conservative why they...   \n3  when you ask the average conservative why they...   \n4  when you ask the average conservative why they...   \n\n                                                body  \\\n0  This post has been locked due to a large numbe...   \n1  I'll share an anecdote. I'm well read enough t...   \n2  Please forgive not having links. I wrote this ...   \n3  You’re not quite thinking about this the right...   \n4  [https://cei.org/blog/wrong-again-50-years-of-...   \n\n                                        cleaned_body  \n0  this post has been locked due to a large numbe...  \n1  i'll share an anecdote. i'm well read enough t...  \n2  please forgive not having links. i wrote this ...  \n3  youre not quite thinking about this the right ...  \n4  how do you tell the difference between actual ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>cleaned_text</th>\n      <th>body</th>\n      <th>cleaned_body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>When you ask the average conservative why they...</td>\n      <td>when you ask the average conservative why they...</td>\n      <td>This post has been locked due to a large numbe...</td>\n      <td>this post has been locked due to a large numbe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>When you ask the average conservative why they...</td>\n      <td>when you ask the average conservative why they...</td>\n      <td>I'll share an anecdote. I'm well read enough t...</td>\n      <td>i'll share an anecdote. i'm well read enough t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>When you ask the average conservative why they...</td>\n      <td>when you ask the average conservative why they...</td>\n      <td>Please forgive not having links. I wrote this ...</td>\n      <td>please forgive not having links. i wrote this ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>When you ask the average conservative why they...</td>\n      <td>when you ask the average conservative why they...</td>\n      <td>You’re not quite thinking about this the right...</td>\n      <td>youre not quite thinking about this the right ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>When you ask the average conservative why they...</td>\n      <td>when you ask the average conservative why they...</td>\n      <td>[https://cei.org/blog/wrong-again-50-years-of-...</td>\n      <td>how do you tell the difference between actual ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# checking the relevant post for our subject >\n# Define keywords related to climate change\n\nimport re\n\n\nkeywords = [\n    \"climate change\", \"global warming\", \"climate crisis\", \"CO2 emissions\",\n    \"fossil fuels\", \"COP\", \"IPCC\", \"climate action\", \"renewable energy\",\n    \"carbon footprint\", \"greenhouse gases\", \"sustainability\", \"carbon neutrality\",\n    \"deforestation\", \"sea level rise\", \"methane emissions\", \"climate adaptation\"\n]\n\n# Create a regex pattern for the keywords\nkeywords_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, keywords)) + r')\\b', re.IGNORECASE)\n\n# Check if the cleaned_text contains any of the keywords\ndata['is_climate_related'] = data['cleaned_text'].apply(\n    lambda x: bool(keywords_pattern.search(x)) if isinstance(x, str) else False\n)\n\n# Verify the results\ntotal_posts = data['link_id'].nunique()\nrelated_posts = data[data['is_climate_related']]['link_id'].nunique()\nunrelated_posts = total_posts - related_posts\n\nprint(f\"Total posts: {total_posts}\")\nprint(f\"Posts related to climate change: {related_posts}\")\nprint(f\"Posts not related to climate change: {unrelated_posts}\")\n\n# Save unrelated posts to a new dataset\nunrelated_data = data[data['is_climate_related'] == False]\nunrelated_data.to_csv('unrelated_posts.csv', index=False)\n\nprint(\"Unrelated posts dataset saved as 'unrelated_posts.csv'\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:04:26.108163Z","iopub.execute_input":"2024-12-09T21:04:26.109280Z","iopub.status.idle":"2024-12-09T21:04:41.141938Z","shell.execute_reply.started":"2024-12-09T21:04:26.109225Z","shell.execute_reply":"2024-12-09T21:04:41.140363Z"}},"outputs":[{"name":"stdout","text":"Total posts: 331\nPosts related to climate change: 227\nPosts not related to climate change: 104\nUnrelated posts dataset saved as 'unrelated_posts.csv'\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# checking how many comments will be remain if we drop irrelevant posts: \n\n# Copy the original dataset to a new DataFrame\ntestdata = data.copy()\n\n# Filter out unrelated posts\ntestdata = testdata[testdata['is_climate_related']]\n\n# Calculate the number of unique posts and comments in the filtered dataset\nremaining_posts = testdata['link_id'].nunique()\nremaining_comments = len(testdata)\n\n# Display the results\nprint(f\"Remaining posts: {remaining_posts}\")\nprint(f\"Remaining comments: {remaining_comments}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:04:41.143592Z","iopub.execute_input":"2024-12-09T21:04:41.144093Z","iopub.status.idle":"2024-12-09T21:04:41.262472Z","shell.execute_reply.started":"2024-12-09T21:04:41.144043Z","shell.execute_reply":"2024-12-09T21:04:41.260933Z"}},"outputs":[{"name":"stdout","text":"Remaining posts: 227\nRemaining comments: 34390\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"len(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:04:41.265921Z","iopub.execute_input":"2024-12-09T21:04:41.266370Z","iopub.status.idle":"2024-12-09T21:04:41.274072Z","shell.execute_reply.started":"2024-12-09T21:04:41.266335Z","shell.execute_reply":"2024-12-09T21:04:41.272890Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"53983"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"data.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:04:41.275875Z","iopub.execute_input":"2024-12-09T21:04:41.276302Z","iopub.status.idle":"2024-12-09T21:04:41.309293Z","shell.execute_reply.started":"2024-12-09T21:04:41.276267Z","shell.execute_reply":"2024-12-09T21:04:41.307646Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"  comment_id  link_id   subreddit_x   parent_id         redditor_id_x  \\\n0    k3d3suw  16yqq5w  changemyview  t3_16yqq5w                10qxmu   \n1    k3anc3z  16yqq5w  changemyview  t3_16yqq5w               deleted   \n2    k3aek0j  16yqq5w  changemyview  t3_16yqq5w                 5kww6   \n3    k3a4ekd  16yqq5w  changemyview  t3_16yqq5w                 4bnx7   \n4    k39xcxh  16yqq5w  changemyview  t3_16yqq5w  suspended:barbodelli   \n\n                created_at_x  \\\n0  2023-10-04T02:32:48+00:00   \n1  2023-10-03T17:34:08+00:00   \n2  2023-10-03T16:40:00+00:00   \n3  2023-10-03T15:32:05+00:00   \n4  2023-10-03T14:38:43+00:00   \n\n                                                body  \\\n0  This post has been locked due to a large numbe...   \n1  I'll share an anecdote. I'm well read enough t...   \n2  Please forgive not having links. I wrote this ...   \n3  You’re not quite thinking about this the right...   \n4  [https://cei.org/blog/wrong-again-50-years-of-...   \n\n                        score_x  edited_x removed_x  ...  \\\n0    {'2024-10-28T14:52:25': 1}     False       NaN  ...   \n1   {'2024-10-28T14:52:26': 12}      True       NaN  ...   \n2   {'2024-10-28T14:52:26': 23}     False       NaN  ...   \n3   {'2024-10-28T14:52:26': 67}      True       NaN  ...   \n4  {'2024-10-28T14:52:27': 235}     False       NaN  ...   \n\n                   upvote_ratio                  num_comments edited_y  \\\n0  {'2024-10-27T12:01:35': 0.8}  {'2024-10-27T12:01:35': 991}     True   \n1  {'2024-10-27T12:01:35': 0.8}  {'2024-10-27T12:01:35': 991}     True   \n2  {'2024-10-27T12:01:35': 0.8}  {'2024-10-27T12:01:35': 991}     True   \n3  {'2024-10-27T12:01:35': 0.8}  {'2024-10-27T12:01:35': 991}     True   \n4  {'2024-10-27T12:01:35': 0.8}  {'2024-10-27T12:01:35': 991}     True   \n\n  archived removed_y poll has_delta  \\\n0     True     False  NaN         0   \n1     True     False  NaN         0   \n2     True     False  NaN         0   \n3     True     False  NaN         0   \n4     True     False  NaN         0   \n\n                                        cleaned_text  \\\n0  when you ask the average conservative why they...   \n1  when you ask the average conservative why they...   \n2  when you ask the average conservative why they...   \n3  when you ask the average conservative why they...   \n4  when you ask the average conservative why they...   \n\n                                        cleaned_body is_climate_related  \n0  this post has been locked due to a large numbe...               True  \n1  i'll share an anecdote. i'm well read enough t...               True  \n2  please forgive not having links. i wrote this ...               True  \n3  youre not quite thinking about this the right ...               True  \n4  how do you tell the difference between actual ...               True  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_id</th>\n      <th>link_id</th>\n      <th>subreddit_x</th>\n      <th>parent_id</th>\n      <th>redditor_id_x</th>\n      <th>created_at_x</th>\n      <th>body</th>\n      <th>score_x</th>\n      <th>edited_x</th>\n      <th>removed_x</th>\n      <th>...</th>\n      <th>upvote_ratio</th>\n      <th>num_comments</th>\n      <th>edited_y</th>\n      <th>archived</th>\n      <th>removed_y</th>\n      <th>poll</th>\n      <th>has_delta</th>\n      <th>cleaned_text</th>\n      <th>cleaned_body</th>\n      <th>is_climate_related</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>k3d3suw</td>\n      <td>16yqq5w</td>\n      <td>changemyview</td>\n      <td>t3_16yqq5w</td>\n      <td>10qxmu</td>\n      <td>2023-10-04T02:32:48+00:00</td>\n      <td>This post has been locked due to a large numbe...</td>\n      <td>{'2024-10-28T14:52:25': 1}</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>{'2024-10-27T12:01:35': 0.8}</td>\n      <td>{'2024-10-27T12:01:35': 991}</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>when you ask the average conservative why they...</td>\n      <td>this post has been locked due to a large numbe...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>k3anc3z</td>\n      <td>16yqq5w</td>\n      <td>changemyview</td>\n      <td>t3_16yqq5w</td>\n      <td>deleted</td>\n      <td>2023-10-03T17:34:08+00:00</td>\n      <td>I'll share an anecdote. I'm well read enough t...</td>\n      <td>{'2024-10-28T14:52:26': 12}</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>{'2024-10-27T12:01:35': 0.8}</td>\n      <td>{'2024-10-27T12:01:35': 991}</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>when you ask the average conservative why they...</td>\n      <td>i'll share an anecdote. i'm well read enough t...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>k3aek0j</td>\n      <td>16yqq5w</td>\n      <td>changemyview</td>\n      <td>t3_16yqq5w</td>\n      <td>5kww6</td>\n      <td>2023-10-03T16:40:00+00:00</td>\n      <td>Please forgive not having links. I wrote this ...</td>\n      <td>{'2024-10-28T14:52:26': 23}</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>{'2024-10-27T12:01:35': 0.8}</td>\n      <td>{'2024-10-27T12:01:35': 991}</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>when you ask the average conservative why they...</td>\n      <td>please forgive not having links. i wrote this ...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>k3a4ekd</td>\n      <td>16yqq5w</td>\n      <td>changemyview</td>\n      <td>t3_16yqq5w</td>\n      <td>4bnx7</td>\n      <td>2023-10-03T15:32:05+00:00</td>\n      <td>You’re not quite thinking about this the right...</td>\n      <td>{'2024-10-28T14:52:26': 67}</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>{'2024-10-27T12:01:35': 0.8}</td>\n      <td>{'2024-10-27T12:01:35': 991}</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>when you ask the average conservative why they...</td>\n      <td>youre not quite thinking about this the right ...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>k39xcxh</td>\n      <td>16yqq5w</td>\n      <td>changemyview</td>\n      <td>t3_16yqq5w</td>\n      <td>suspended:barbodelli</td>\n      <td>2023-10-03T14:38:43+00:00</td>\n      <td>[https://cei.org/blog/wrong-again-50-years-of-...</td>\n      <td>{'2024-10-28T14:52:27': 235}</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>{'2024-10-27T12:01:35': 0.8}</td>\n      <td>{'2024-10-27T12:01:35': 991}</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>when you ask the average conservative why they...</td>\n      <td>how do you tell the difference between actual ...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Filter for comments with a delta after cleaned_body is added\ndelta_comments = data[data['has_delta'] == 1]\nprint(\"number of commens has delta\",len(delta_comments))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:04:41.311024Z","iopub.execute_input":"2024-12-09T21:04:41.311603Z","iopub.status.idle":"2024-12-09T21:04:41.328098Z","shell.execute_reply.started":"2024-12-09T21:04:41.311548Z","shell.execute_reply":"2024-12-09T21:04:41.326512Z"}},"outputs":[{"name":"stdout","text":"number of commens has delta 918\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"data['body'][1230]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:04:41.329750Z","iopub.execute_input":"2024-12-09T21:04:41.330241Z","iopub.status.idle":"2024-12-09T21:04:41.346906Z","shell.execute_reply.started":"2024-12-09T21:04:41.330192Z","shell.execute_reply":"2024-12-09T21:04:41.345868Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'!delta\\n\\nI liked the way you worded that- it’s not that people are secretly desperately wanted to have children but lack $3k and thus giving them $3k would make them want to give birth- rather that people have all sorts of reasons in their personal scales and adding a little incentive here or there it’s enough to tip the scale for some people.\\n\\nThat said the article you attached did say it increased the birth rate by 4-6% not 60%.'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"data['cleaned_body'][1230]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:04:41.348342Z","iopub.execute_input":"2024-12-09T21:04:41.348727Z","iopub.status.idle":"2024-12-09T21:04:41.363607Z","shell.execute_reply.started":"2024-12-09T21:04:41.348695Z","shell.execute_reply":"2024-12-09T21:04:41.362282Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'!delta i liked the way you worded that- its not that people are secretly desperately wanted to have children but lack 3k and thus giving them 3k would make them want to give birth- rather that people have all sorts of reasons in their personal scales and adding a little incentive here or there its enough to tip the scale for some people. that said the article you attached did say it increased the birth rate by 4-6% not 60%.'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"#NOT \n# this code is for glove and the  other embedding :\n\n\nimport re\nimport pandas as pd\nfrom contractions import contractions_dict  # You need a dictionary of common contractions\n\n# Define a function to expand contractions\ndef expand_contractions(text):\n    contraction_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n    return contraction_pattern.sub(lambda match: contractions_dict[match.group(0)], text)\n\n# Contractions dictionary (You can expand this as needed)\ncontractions_dict = {\n    \"don't\": \"do not\",\n    \"you're\": \"you are\",\n    \"it's\": \"it is\",\n    \"they're\": \"they are\",\n    \"we're\": \"we are\",\n    \"I've\": \"I have\",\n    \"can't\": \"cannot\",\n    \"isn't\": \"is not\",\n    \"didn't\": \"did not\",\n    \"won't\": \"will not\",\n    \"wouldn't\": \"would not\",\n    \"couldn't\": \"could not\",\n    \"shouldn't\": \"should not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    # Add more contractions here\n}\n\n# Extended cleaning function\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Expand contractions\n    text = expand_contractions(text)\n    \n    # Handle common encoding issues\n    text = text.replace(\"â€™\", \"'\").replace(\"â€œ\", '').replace(\"â€\", '')\n    text = text.replace(\"â€“\", \"-\").replace(\"â€¦\", \"...\").replace(\"â€\", \"\")\n    \n    # Remove URLs\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n    \n    # Remove HTML tags\n    text = re.sub(r\"<.*?>\", '', text)\n    \n    # Remove email addresses\n    text = re.sub(r'\\S+@\\S+', '', text)\n    \n    # Remove mentions (e.g., @username)\n    text = re.sub(r\"@\\w+\", '', text)\n    \n    # Remove special characters but retain mathematical symbols, percentages, and numbers\n    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?%'():;+-/]\", '', text)\n    \n    # Normalize multiple spaces\n    text = re.sub(r\"\\s+\", ' ', text).strip()\n\n    # Replace common escape sequences with their intended characters\n    text = text.replace(\"\\\\'\", \"'\")  # Fix escaped single quotes\n    \n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n    text = text.strip()  # Remove leading/trailing spaces  \n    \n    return text\n\n# Apply cleaning to the `text` and `body` columns\ndata['cleaned_text'] = data['text'].apply(clean_text)\ndata['cleaned_body'] = data['body'].apply(clean_text)\n\n# Display a sample of the cleaned data\ndata[['text', 'cleaned_text', 'body', 'cleaned_body']].head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optional: Save the updated dataset\n# data.to_csv('/kaggle/working/data1.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\n# Filter rows where the cleaned_body contains \"!delta\"\ndelta_comments = data[data['cleaned_body'].str.contains(r'\\!delta', case=False, na=False)]\n\nprint(delta_comments.body.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:04:41.365638Z","iopub.execute_input":"2024-12-09T21:04:41.366136Z","iopub.status.idle":"2024-12-09T21:04:41.451018Z","shell.execute_reply.started":"2024-12-09T21:04:41.366088Z","shell.execute_reply":"2024-12-09T21:04:41.449541Z"}},"outputs":[{"name":"stdout","text":"210     Okay, that's a fair point. I really should cla...\n1230    !delta\\n\\nI liked the way you worded that- it’...\n1246    !delta\\n\\nThis right here is the summary of wh...\n1265    >It seems to me like even if you personally do...\n1278    !delta\\n\\na lot of people see \"work\" as their ...\n1280    I'm not OP but I have the same opinion so !del...\n1298    >Nowadays it’s way more common for women over ...\n1305    !delta\\n\\nI have changed my view to \"a big rea...\n1322    >My wife and I had two.  If we owned our own h...\n1482    The person you replied to interpreted your sta...\nName: body, dtype: object\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NOT\n\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords', quiet=True)\n\ndef clean_reddit_comments(data, text_column='cleaned_body'):\n    \"\"\"\n    Clean Reddit comments by:\n    1. Removing URLs\n    2. Removing Reddit-specific metadata\n    3. Removing very short or non-meaningful comments\n    \n    Parameters:\n    -----------\n    data : pandas.DataFrame\n        Input DataFrame containing comments\n    text_column : str, optional\n        Name of the column containing comment text (default: 'cleaned_body')\n    \n    Returns:\n    --------\n    pandas.DataFrame\n        Cleaned DataFrame with filtered comments\n    \"\"\"\n    # Download stopwords if not already downloaded\n    try:\n        stop_words = set(stopwords.words('english'))\n    except LookupError:\n        nltk.download('stopwords', quiet=True)\n        stop_words = set(stopwords.words('english'))\n    \n    # Function to clean individual comments\n    def clean_comment(comment):\n        if not isinstance(comment, str):\n            return None\n        \n        # Remove URLs\n        comment = re.sub(r'http\\S+|www\\.\\S+', '', comment)\n        \n        # Remove delta confirmation lines while preserving the actual comment\n        comment = re.sub(r'^Confirmed:.*?to /u/\\w+\\s*', '', comment, flags=re.IGNORECASE)\n        \n        # Remove superscript text\n        comment = re.sub(r'\\^.*', '', comment)\n        \n        # Remove !delta and other Reddit-specific markers\n        comment = re.sub(r'!delta', '', comment)\n        \n        # Remove user mentions\n        comment = re.sub(r'/u/\\w+', '', comment)\n        \n        # Remove special characters and extra whitespace\n        comment = re.sub(r'[^a-zA-Z\\s]', '', comment)\n        \n        # Trim and normalize whitespace\n        comment = ' '.join(comment.split())\n        \n        return comment\n    \n    # Create a copy of the dataframe to avoid modifying the original\n    cleaned_data = data.copy()\n    \n    # Apply cleaning to the specified column\n    cleaned_data['cleaned_comments'] = cleaned_data[text_column].apply(clean_comment)\n    \n    # Filter out comments that are too short or empty\n    def is_meaningful_comment(comment):\n        if not isinstance(comment, str):\n            return False\n        \n        # Split into words\n        words = comment.split()\n        \n        # Check if comment has meaningful content\n        return (len(words) >= 3 and \n                any(len(word) > 2 for word in words))\n    \n    # Filter the DataFrame\n    meaningful_comments = cleaned_data[cleaned_data['cleaned_comments'].apply(is_meaningful_comment)].copy()\n    \n    # Optional: Reset index\n    meaningful_comments.reset_index(drop=True, inplace=True)\n    \n    # Diagnostic information\n    print(f\"Original number of comments: {len(data)}\")\n    print(f\"Number of cleaned meaningful comments: {len(meaningful_comments)}\")\n    print(f\"Comments removed: {len(data) - len(meaningful_comments)}\")\n    \n    return meaningful_comments\n\n# Clean the comments\ncleaned_data = clean_reddit_comments(data)\n\n# Optionally, you might want to save the cleaned data\n# cleaned_data.to_csv('cleaned_comments.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NOT : \n\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords', quiet=True)\n\ndef clean_and_filter_comments(data):\n    \"\"\"\n    Clean and filter comments in the DataFrame:\n    1. Remove URLs and metadata\n    2. Remove meaningless comments\n    3. Remove null/empty rows\n    \n    Parameters:\n    -----------\n    data : pandas.DataFrame\n        Input DataFrame containing comments\n    \n    Returns:\n    --------\n    pandas.DataFrame\n        Cleaned and filtered DataFrame\n    \"\"\"\n    # Print initial state\n    initial_rows = len(data)\n    initial_null_rows = data['cleaned_body'].isnull().sum()\n    \n    # Download stopwords if not already downloaded\n    try:\n        stop_words = set(stopwords.words('english'))\n    except LookupError:\n        nltk.download('stopwords', quiet=True)\n        stop_words = set(stopwords.words('english'))\n    \n    # Function to clean individual comments\n    def clean_comment(comment):\n        if not isinstance(comment, str):\n            return None\n        \n        # Remove URLs\n        comment = re.sub(r'http\\S+|www\\.\\S+', '', comment)\n        \n        # Remove delta confirmation lines while preserving the actual comment\n        comment = re.sub(r'^Confirmed:.*?to /u/\\w+\\s*', '', comment, flags=re.IGNORECASE)\n        \n        # Remove superscript text\n        comment = re.sub(r'\\^.*', '', comment)\n        \n        # Remove !delta and other Reddit-specific markers\n        comment = re.sub(r'!delta', '', comment)\n        \n        # Remove user mentions\n        comment = re.sub(r'/u/\\w+', '', comment)\n        \n        # Remove special characters and extra whitespace\n        comment = re.sub(r'[^a-zA-Z\\s]', '', comment)\n        \n        # Trim and normalize whitespace\n        comment = ' '.join(comment.split())\n        \n        return comment\n    \n    # Clean comments\n    data['cleaned_body'] = data['cleaned_body'].apply(clean_comment)\n    \n    # Check if comment is meaningful\n    def is_meaningful_comment(comment):\n        if not isinstance(comment, str):\n            return False\n        \n        # Split into words\n        words = comment.split()\n        \n        # Check if comment has meaningful content\n        return (len(words) >= 3 and \n                any(len(word) > 2 for word in words))\n    \n    # Create masks\n    meaningful_mask = data['cleaned_body'].apply(is_meaningful_comment)\n    null_mask = data['cleaned_body'].isnull()\n    \n    # Remove meaningless comments and null rows\n    data.drop(data[~meaningful_mask | null_mask].index, inplace=True)\n    \n    # Reset index\n    data.reset_index(drop=True, inplace=True)\n    \n    # Print detailed results\n    final_rows = len(data)\n    print(\"Cleaning Statistics:\")\n    print(f\"Initial number of rows: {initial_rows}\")\n    print(f\"Initial null rows in cleaned_body: {initial_null_rows}\")\n    print(f\"Final number of rows: {final_rows}\")\n    print(f\"Rows removed: {initial_rows - final_rows}\")\n    print(f\"Percentage of rows removed: {((initial_rows - final_rows)/initial_rows)*100:.2f}%\")\n    \n    return data\n\n# Apply the cleaning function to your dataframe\nclean_and_filter_comments(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# run this one\n\n\n\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords', quiet=True)\n\ndef clean_and_filter_comments(data):\n    \"\"\"\n    Clean and filter comments in the DataFrame with improved preservation:\n    1. Preserve punctuation and apostrophes\n    2. Keep percentages and numbers\n    3. Remove URLs and metadata\n    4. Remove meaningless comments\n    5. Remove null/empty rows\n    \n    Parameters:\n    -----------\n    data : pandas.DataFrame\n        Input DataFrame containing comments\n    \n    Returns:\n    --------\n    pandas.DataFrame\n        Cleaned and filtered DataFrame\n    \"\"\"\n    # Print initial state\n    initial_rows = len(data)\n    initial_null_rows = data['cleaned_body'].isnull().sum()\n    \n    # Download stopwords if not already downloaded\n    try:\n        stop_words = set(stopwords.words('english'))\n    except LookupError:\n        nltk.download('stopwords', quiet=True)\n        stop_words = set(stopwords.words('english'))\n    \n    # Function to clean individual comments\n    def clean_comment(comment):\n        if not isinstance(comment, str):\n            return None\n        \n        # Remove URLs\n        #comment = re.sub(r'http\\S+|www\\.\\S+', '', comment)\n        \n        # Remove delta confirmation lines while preserving the actual comment\n        #comment = re.sub(r'^Confirmed:.*?to /u/\\w+\\s*', '', comment, flags=re.IGNORECASE)\n        \n        # Remove superscript text\n        #comment = re.sub(r'\\^.*', '', comment)\n        \n        # Remove !delta and other Reddit-specific markers\n        #comment = re.sub(r'!delta', '', comment)\n        \n        # Remove user mentions\n        #comment = re.sub(r'/u/\\w+', '', comment)\n        \n        # Remove excessive whitespace while preserving essential punctuation\n        #comment = re.sub(r'\\s+', ' ', comment.strip())\n\n\n        # Comprehensive pattern removal\n        patterns_to_remove = [\n        # URLs and web-related patterns\n        r'http\\S+|www\\.\\S+',  # URLs\n        r'\\S+\\.(com|org|net|edu|gov)\\S*',  # Domain patterns\n        \n        # Reddit-specific patterns\n        r'^Confirmed:.*?to /u/\\w+\\s*',  # Delta confirmation lines\n        r'/u/\\w+',  # User mentions\n        r'!delta',  # Delta markers\n        r'!Delta',  # Delta markers\n            \n        \n        # Superscript and complex text patterns\n        r'\\^.*',  # Superscript text\n        r'\\(.*?/r/\\w+/wiki/user/\\w+.*?\\)',  # Wiki user links\n        r'delta\\s*system\\s*explained',  # Delta system explanations\n        r'deltaboards',  # Deltaboards references\n        \n        # Parenthetical numeric patterns\n        r'\\((\\d+)\\)',  # Numeric content in parentheses\n        r'^[\\(\\)\\d\\s]+$',  # Lines with only parentheses, digits, spaces\n        \n        # Special markdown and formatting\n        r'\\[.*?\\]',  # Remove markdown link text\n        r'\\(.*?\\)',  # Remove markdown link URLs\n        ]\n    \n        # Apply all removal patterns\n        for pattern in patterns_to_remove:\n            comment = re.sub(pattern, '', comment, flags=re.IGNORECASE)\n            # Remove excessive whitespace while preserving essential punctuation\n            comment = re.sub(r'\\s+', ' ', comment.strip())\n            return comment\n    \n    # Clean comments\n    data['cleaned_body'] = data['cleaned_body'].apply(clean_comment)\n    \n    # Check if comment is meaningful\n    def is_meaningful_comment(comment):\n        if not isinstance(comment, str):\n            return False\n        \n        # Split into words\n        words = comment.split()\n        \n        # Check if comment has meaningful content\n        return (len(words) >= 3 and \n                any(len(word.strip('.,!?%')) > 2 for word in words))\n    \n    # Create masks\n    meaningful_mask = data['cleaned_body'].apply(is_meaningful_comment)\n    null_mask = data['cleaned_body'].isnull()\n    \n    # Remove meaningless comments and null rows\n    data.drop(data[~meaningful_mask | null_mask].index, inplace=True)\n    \n    # Reset index\n    data.reset_index(drop=True, inplace=True)\n    \n    # Print detailed results\n    final_rows = len(data)\n    print(\"Cleaning Statistics:\")\n    print(f\"Initial number of rows: {initial_rows}\")\n    print(f\"Initial null rows in cleaned_body: {initial_null_rows}\")\n    print(f\"Final number of rows: {final_rows}\")\n    print(f\"Rows removed: {initial_rows - final_rows}\")\n    print(f\"Percentage of rows removed: {((initial_rows - final_rows)/initial_rows)*100:.2f}%\")\n    \n    return data\n\n# Let's also add a function to show some examples of preserved elements\ndef show_cleaning_examples(data, n=5):\n    \"\"\"\n    Show examples of how comments were cleaned\n    \n    Parameters:\n    -----------\n    data : pandas.DataFrame\n        Cleaned DataFrame\n    n : int, optional\n        Number of examples to show\n    \"\"\"\n    print(\"\\nCleaning Examples:\")\n    sample_indices = data.sample(min(n, len(data))).index\n    \n    for idx in sample_indices:\n        print(\"\\nOriginal Comment:\")\n        print(data.loc[idx, 'cleaned_body'])\n\n# Apply the cleaning function to your dataframe\nclean_and_filter_comments(data)\n\n# Optional: Show some cleaning examples\nshow_cleaning_examples(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:11:40.552852Z","iopub.execute_input":"2024-12-09T21:11:40.554357Z","iopub.status.idle":"2024-12-09T21:11:44.085589Z","shell.execute_reply.started":"2024-12-09T21:11:40.554290Z","shell.execute_reply":"2024-12-09T21:11:44.084044Z"}},"outputs":[{"name":"stdout","text":"Cleaning Statistics:\nInitial number of rows: 53983\nInitial null rows in cleaned_body: 0\nFinal number of rows: 51209\nRows removed: 2774\nPercentage of rows removed: 5.14%\n\nCleaning Examples:\n\nOriginal Comment:\nthe other aspect of this is most people here are depicting'flood risk'as a binary - either you've got it or you don't. everyone has flood risk. everyone. some more than others, for certain, but we all have it. furthermore, 100 year flood plains are considered the'high risk'areas. you're not allowed to build there and you can't get insurance if you do. however, it's not like 100 year flood plains are static. people build developments, the landscape changes, more road, buildings, and parking lots (known as'hardscaping') expand the flood plain. these things have to be re-assessed frequently and fema doesn't have the money to do it everywhere. people who don't live in a 100 year flood plain as the previous maps suddenly find themselves in a flood plain years later because someone built up an area near them. on top of that, climate change is making 500 and 1,000 year flood plains happen more frequently. that means people who are'no where near'the flood plain are getting flooded because the flood event was so much worse than before. this stuff is a lot more dynamic than people like to admit.\n\nOriginal Comment:\nyou keep making that claim but haven't actually justified it. your evidence is your feelings which is not rational because all of the data says otherwise.\n\nOriginal Comment:\nthere is another simple explanation. the basic premises are simple and the supporting data is sound. humans pour insane amounts of c02 into atmosphere. we put enough into the atmosphere that it is changing the climate of the planet. glaciers are melting and have melted. the arctic ocean has measuraby less ice over it (max in winter and min in summer). global average temperatures and ocean temperatures have rising. corals are bleaching en masse. we are impacting the climate of our planet and it's mostly negative. secondarily we are harming the planet directly as well. we clear-cut forests of old growth for agriculture. we divide most large areas where wildlife could live with roads. there is a pile of garbage floating in the middle of the pacific bugger than some states. the number of species we have forced to extinction is astounding. we have polluted chemically and nuclearly by accident and on purpose. we harm the planet in a lot of ways. some people see this as less of a problem than others. there is much potential debate to be had on principle values of progress vs preservation. there isn't much debate to be had on how much co2 is emitted and how much that impacts the climate. 98% of climate scientists or whatever agree. the experts are in consensus. that's not herd behavior that's just the facts after the experts settled (most of) their debates. the real debate is on whether we actually care about that impact and how much of a problem will actually manifest. climate scientists can't predict everything. they can only predict the climate. perhaps some liberties have been taken. i sympathize that there is too much level of alarmism. honestly we need to just take better care of the planet for the sake of taking better care of the planet. climate scientists can't predict how some species will adapt to changing climates. they can't really predict how humans will react to the change in climate either. all of their worst fears for some prediction made could come through and we could just weather it out. in fact whatever comes we will weather it. droughts will come. people will be relocated and rivers will be diverted. forest fires will burn. people will be relocated and such. corals will bleach; not all of them will die but the total amount of coral will be reduced. biodiversity will decrease. all of the stuff will happen and we will weather it. there won't be some climate apocalypse. there won't be a day where everything changes. floods fires and droughts do that to individuals but that already happens. it's just going to happen more. we will pay the price day by day and year by year by seeing an increase in frequency and severity of many difficulties and challenges already faced by people. so basically we can pay to invest in a better future, or we can take the easy way now and pay the upkeep for it later. there won't be some climate apocalypse. there will just be an increasing upkeep cost which will eventually result in some geopolitical event. water scarcity or whatever might the basis of conflict but it won't be the climate wars not to us. some future star trek civilization might look back and be able to say that. to us it'll just be war or some cold conflict where the most militarized nations just hang around in the countries with all the water. like already have international occupations by nato all over the world. we peacekeep everywhere. you honestly think securing water sources will look much different if large scale open conflict were avoided... you honestly think there are no conflicts in the world over water sources already?!\n\nOriginal Comment:\nneither is medicine constrained by simplicity. hence it is better if experts manage it, not laypeople.\n\nOriginal Comment:\ni simply cant decide how specifically applicable your comments are. i personally have no insight into the american culture regarding secondary education (which i assume is what is likely being discussed here), but its normal in the country im from for high school students to be educated in statistical and scientific writing, and as a graduation requirement were made to complete and defend a research paper (which has to be good enough to publish in order for us to pass). certainly a lot of students who underwent that process struggled with it, but in my graduating class and in the preceding and succeeding classes, not a single student failed out because of it. they were made to learn it, and at least partially, did. i dont know if culturally thats an impossible expectation of american children, so if my lack of understanding is there i apologize.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# NOT\n\n\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords', quiet=True)\n\ndef clean_and_filter_comments(data):\n    \"\"\"\n    Clean and filter comments in the DataFrame with improved preservation:\n    1. Preserve punctuation and apostrophes\n    2. Keep percentages and numbers\n    3. Remove URLs and metadata\n    4. Remove meaningless comments\n    5. Remove null/empty rows\n    \n    Parameters:\n    -----------\n    data : pandas.DataFrame\n        Input DataFrame containing comments\n    \n    Returns:\n    --------\n    pandas.DataFrame\n        Cleaned and filtered DataFrame\n    \"\"\"\n    # Print initial state\n    initial_rows = len(data)\n    initial_null_rows = data['cleaned_body'].isnull().sum()\n    \n    # Download stopwords if not already downloaded\n    try:\n        stop_words = set(stopwords.words('english'))\n    except LookupError:\n        nltk.download('stopwords', quiet=True)\n        stop_words = set(stopwords.words('english'))\n    \n    # Function to clean individual comments\n    def clean_comment(comment):\n        if not isinstance(comment, str):\n            return None\n        \n        # Comprehensive pattern removal\n        patterns_to_remove = [\n            # URLs and web-related patterns\n            r'http\\S+|www\\.\\S+',  # URLs\n            r'\\S+\\.(com|org|net|edu|gov)\\S*',  # Domain patterns\n        \n            # Reddit-specific patterns\n            r'^Confirmed:.*?to /u/\\w+\\s*',  # Delta confirmation lines\n            r'/u/\\w+',  # User mentions\n            r'!delta',  # Delta markers\n            r'!Delta',  # Delta markers\n                \n            # Superscript and complex text patterns\n            r'\\^.*',  # Superscript text\n            r'\\(.*?/r/\\w+/wiki/user/\\w+.*?\\)',  # Wiki user links\n            r'delta\\s*system\\s*explained',  # Delta system explanations\n            r'deltaboards',  # Deltaboards references\n        \n            # Parenthetical numeric patterns\n            r'\\((\\d+)\\)',  # Numeric content in parentheses\n            r'^[\\(\\)\\d\\s]+$',  # Lines with only parentheses, digits, spaces\n            \n            # Special markdown and formatting\n            r'\\[.*?\\]',  # Remove markdown link text\n            r'\\(.*?\\)',  # Remove markdown link URLs\n        ]  \n        \n        # Apply all removal patterns\n        for pattern in patterns_to_remove:\n            comment = re.sub(pattern, '', comment, flags=re.IGNORECASE)\n        \n        # Remove excessive whitespace while preserving essential punctuation\n        comment = re.sub(r'\\s+', ' ', comment.strip())\n\n        return comment\n    \n    # Clean comments\n    data['cleaned_body'] = data['cleaned_body'].apply(clean_comment)\n    \n    # Check if comment is meaningful\n    def is_meaningful_comment(comment):\n        if not isinstance(comment, str):\n            return False\n        # Remove comments with non-informative patterns (garbled text)\n        if re.match(r'^[-.,()\\s]*$', comment) or re.match(r'^[-\\w\\d\\s().,/]+$', comment.strip()):\n            return False\n        \n        # Split into words\n        words = comment.split()\n        \n        # Check if comment has meaningful content\n        return (len(words) >= 3 and \n                any(len(word.strip('.,!?%')) > 2 for word in words))\n        \n\n    \n\n    \n    \n    # Create masks\n    meaningful_mask = data['cleaned_body'].apply(is_meaningful_comment)\n    null_mask = data['cleaned_body'].isnull()\n    \n    # Remove meaningless comments and null rows\n    data.drop(data[~meaningful_mask | null_mask].index, inplace=True)\n    \n    # Reset index\n    data.reset_index(drop=True, inplace=True)\n    \n    # Print detailed results\n    final_rows = len(data)\n    print(\"Cleaning Statistics:\")\n    print(f\"Initial number of rows: {initial_rows}\")\n    print(f\"Initial null rows in cleaned_body: {initial_null_rows}\")\n    print(f\"Final number of rows: {final_rows}\")\n    print(f\"Rows removed: {initial_rows - final_rows}\")\n    print(f\"Percentage of rows removed: {((initial_rows - final_rows)/initial_rows)*100:.2f}%\")\n    \n    return data\n\n# Function to show some examples of preserved elements\ndef show_cleaning_examples(data, n=5):\n    \"\"\"\n    Show examples of how comments were cleaned\n    \n    Parameters:\n    -----------\n    data : pandas.DataFrame\n        Cleaned DataFrame\n    n : int, optional\n        Number of examples to show\n    \"\"\"\n    print(\"\\nCleaning Examples:\")\n    sample_indices = data.sample(min(n, len(data))).index\n    \n    for idx in sample_indices:\n        print(\"\\nOriginal Comment:\")\n        print(data.loc[idx, 'cleaned_body'])\n\n# Apply the cleaning function to your dataframe\nclean_and_filter_comments(data)\n\n# Optional: Show some cleaning examples\nshow_cleaning_examples(data)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter for comments with a delta after cleaned_body is added\ndelta_comments = data[data['has_delta'] == 1]\nprint(f\"Number of comments with delta: {len(delta_comments)}\")\n\n# \nprint(len(data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:11:56.155168Z","iopub.execute_input":"2024-12-09T21:11:56.155558Z","iopub.status.idle":"2024-12-09T21:11:56.168096Z","shell.execute_reply.started":"2024-12-09T21:11:56.155524Z","shell.execute_reply":"2024-12-09T21:11:56.166879Z"}},"outputs":[{"name":"stdout","text":"Number of comments with delta: 910\n51209\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"delta_comments['cleaned_body'].head(30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:12:19.842252Z","iopub.execute_input":"2024-12-09T21:12:19.842680Z","iopub.status.idle":"2024-12-09T21:12:19.853622Z","shell.execute_reply.started":"2024-12-09T21:12:19.842647Z","shell.execute_reply":"2024-12-09T21:12:19.852297Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"202     okay, that's a fair point. i really should cla...\n1162    !delta i liked the way you worded that- its no...\n1177    !delta this right here is the summary of what ...\n1193    it seems to me like even if you personally don...\n1205    !delta a lot of people see work as their reaso...\n1207    i'm not op but i have the same opinion so !del...\n1222    nowadays its way more common for women over 35...\n1223    confirmed: 1 delta awarded to /u/dollarfrom15c...\n1229    !delta i have changed my view to a big reason ...\n1244    my wife and i had two. if we owned our own hom...\n1263    confirmed: 1 delta awarded to /u/dollarfrom15c...\n1266    confirmed: 1 delta awarded to /u/exis007 (88(/...\n1288    confirmed: 1 delta awarded to /u/nylockian (3(...\n1293    confirmed: 1 delta awarded to /u/nylockian (1(...\n1325    confirmed: 1 delta awarded to /u/lordkristivas...\n1394    the person you replied to interpreted your sta...\n1758    !delta best argument but still kinda disagree....\n1774    !delta your first argument has legs the other ...\n2129    confirmed: 1 delta awarded to /u/iaminthefores...\n2138    !delta you're right. my viewpoints would imple...\n2139    username checks out lol. !delta it's right. sc...\n2142    !delta you're right, it's a complex topic. it'...\n2147    !delta you're right. it's a problem. we have t...\n2158    climate change is a very general topic with no...\n2164    !delta it's a complex topic. we have to do mor...\n2172    !delta you're right. we have to give more info...\n2176    !delta seems like i didn't know enough about c...\n2190    confirmed: 1 delta awarded to /u/denyscience (...\n2193    confirmed: 1 delta awarded to /u/youdamanindah...\n2197    confirmed: 1 delta awarded to /u/cacafuego (10...\nName: cleaned_body, dtype: object"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(data['body'][2245])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:12:20.080654Z","iopub.execute_input":"2024-12-09T21:12:20.081115Z","iopub.status.idle":"2024-12-09T21:12:20.088232Z","shell.execute_reply.started":"2024-12-09T21:12:20.081071Z","shell.execute_reply":"2024-12-09T21:12:20.086774Z"}},"outputs":[{"name":"stdout","text":"Please read [the last paragraph](https://earthobservatory.nasa.gov/world-of-change/global-temperatures) and then explain how the temperature data isn't representative of the globe. Keep in mind other research entities have their own temperature records and have independently verified NASA's findings.\n\nDid you seriously think you, a random layperson, identified a flaw in the research that no one else noticed, and that flaw only required a rudimentary understanding of random sampling? I have to wonder if this variant of climate denial is just bad Mary Sue fan fiction.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(data['cleaned_body'][2245])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:12:22.692738Z","iopub.execute_input":"2024-12-09T21:12:22.693139Z","iopub.status.idle":"2024-12-09T21:12:22.699414Z","shell.execute_reply.started":"2024-12-09T21:12:22.693107Z","shell.execute_reply":"2024-12-09T21:12:22.698151Z"}},"outputs":[{"name":"stdout","text":"please read the last paragraph( and then explain how the temperature data isn't representative of the globe. keep in mind other research entities have their own temperature records and have independently verified nasa's findings. did you seriously think you, a random layperson, identified a flaw in the research that no one else noticed, and that flaw only required a rudimentary understanding of random sampling? i have to wonder if this variant of climate denial is just bad mary sue fan fiction.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(data['text'][3])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:12:23.079399Z","iopub.execute_input":"2024-12-09T21:12:23.079791Z","iopub.status.idle":"2024-12-09T21:12:23.086780Z","shell.execute_reply.started":"2024-12-09T21:12:23.079760Z","shell.execute_reply":"2024-12-09T21:12:23.085138Z"}},"outputs":[{"name":"stdout","text":"When you ask the average conservative why they don't seem to take climate change seriously, you often hear something along the lines of \"well, they've all told us that the world would end in 10 years, and it didn't, and since that didn't happen, my trust is now completely broken and I feel entitled not to have to listen to 'experts' anymore.\"\n\nIt's this bit about \"the experts have said the world will end in 10 years\" that I find highly suspicious, and it is my view that this information likely came from a NON-expert, in a casual context, without citation of actual evidence and research. I think the average conservative just heard one of their friends, someone who isn't a scientist and doesn't have a good grasp of the data or even really know how to explain the science behind climate change, say something like \"dude we gotta stop driving cars or the world will be over in 10 years\" and they just took that and ran with it.\n\nBasically I attribute this to shoddy and irresponsible research on the part of conservatives. Did you verify that these claims came from *actual scientists*? Did these claims come from respected institutions or at least a person with decent credentials, someone with a degree better than a BS and hopefully employed with the likes of, say, NOAA? Because I doubt it.\n\nWhen conservatives say they heard these claims, I just straight-up do not believe that they came from a credible source. I've followed what respected scientists and respected institutions have said about climate change for most of my 38 years of life, and never have I heard them say anything along the lines of the strawmen that conservatives regularly state, things like \"the world will LITERALLY END in X years\" or putting world-ending timelines on a scale of something like a couple of decades. The very foundation of your argument is built on a false premise at best and a complete lie at worst.\n\nKeep in mind, respected scientists HAVE said things along the lines of \"in 10-20 years, we could reach a *point of no return*\", which is NOT the same as saying \"the world is, at this point in time, literally over and done with\", but it IS saying that we've reached a point where it will no longer be possible to save ourselves. We can reach a \"point of no return\" in a timeline of 10-20 years and still have another 100-200 years until the consequences of that become so severe that people start to die simply because of how inhospitable the planet is, but clearly that's saying something very different, right?\n\nConservatives, convince me that you've ever actually been told this doom-and-gloom from a RESPECTED source and I'll take this seriously. But it is currently my view that anyone parroting this line of \"oh but they've told us the WORLD WILL END IN 10-20 YEARS\" is just parroting bullshit that was never said by an expert and is ultimately just a strawman you've conjured up to excuse yourselves from having to take climate change seriously. CMV.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print(data['text'][652])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:12:25.709389Z","iopub.execute_input":"2024-12-09T21:12:25.709871Z","iopub.status.idle":"2024-12-09T21:12:25.716063Z","shell.execute_reply.started":"2024-12-09T21:12:25.709826Z","shell.execute_reply":"2024-12-09T21:12:25.714780Z"}},"outputs":[{"name":"stdout","text":"I've seen the argument of veganism that the meat industry is extremely polluting, however it always seemed like a poor argument. The polluting nature of the meat industry is not an argument against the meat industry itself but rather against the industry's nature of exploitative expansion. \n\nThe fact that meat is produced for the purpose of profit maximization and not for planned human consumption is the key difference between humans consuming meat in prehistoric or early times vs. now. This implies that the problem is not the human tendency of eating meat itself, but rather the way it is being produced and expanding. \n\nIt feels very similar to the whole Reduce, Reuse, Recycle thing. Individual actions of recycling harmful waste does nearly nothing unless the source of said harmful waste isn't mass produced in a planned and advanced manner. \n\nInstead of controlling this profit based production of animal products, corporations will try to push the narrative of veganism down the throats of the populace to give them the illusion of a vegan world, which seems nearly impossible to achieve when considering the basic facets and evolution of human diet. This allows them to wash their hands of their responsibility. \n\nTL:DR; Corporations try to push a vegan utopia to wash their hands of responsibility and place blame on the consumers who simply eat a naturally formed diet of theirs. \n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(data['cleaned_text'][2000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:12:27.001133Z","iopub.execute_input":"2024-12-09T21:12:27.001580Z","iopub.status.idle":"2024-12-09T21:12:27.007911Z","shell.execute_reply.started":"2024-12-09T21:12:27.001544Z","shell.execute_reply":"2024-12-09T21:12:27.006546Z"}},"outputs":[{"name":"stdout","text":"i only say americans because i am one and dont know enough about the culture outside america enough to make judgements. climate change will cause terrible environmental changes that will cause millions to die/suffer from flooding, food shortages, etc. this will come to head in the coming decades. this is a fact. to solve climate change we need to emit less co2 in the atmosphere. we can do that in america: 1. eliminating single use plastics: -buying food at refillerys -using glass and aluminum drinks -companies will be fined/stoped for using wasteful plastic packaging -right to repair will be law 2. banning the use private vehicles: -every has to take the bus, train, or bike (unless they have great reason not to) -the rich cant use private planes -banning new gas vehicles 3. nationalizing, regulating, rationing public utilities: -utilities will be free but rationed depending on need -coal and natural gas power plants will only be used if renewables cant (edit: im obviously down for nuclear) -anyone using too much ultilities will be fined/stopped -helping other countries under americas influence do the same -banning single family detached housing -banning new housing too far outside a urban center doing this in america by itself will mitigate climate change greatly but most americans are too self-involved to give up cars and single family houses. edit: -banning new single family detached housing -for private vehicles small scale electric recreation vehicles are allowed edit 2: wow, this post got a lot of attention. most people got caught up in the car thing. i specifically said unless you have a great reason. i didnt say we are rounding up everyones cars and destroying them. just the idea of using public transportation made many people visceral angry. (i also dont appreciate the insults in my messages) that proved my point, many people wouldnt change for the good of society. these measures if taken seriously would have to be done in like 10 year timeline. but for them to be done at all some people will have to sacrifice their time and comfort. most people rather say climate change isnt real or china is the problem. before we criticize other countries lets look at our selves. i understand we need regulations to fully do these things but why would regulations come if it looks like nobody cares. also not all these things have to be done right away at one time. long story short the sentiment i got from everyone is: fuck you, im not changing it makes my life harder instead of those solutions are drastic but im going to start making changes so we start a movement that starts mitigating the worse effects of climate change and pollution. and that was my problem p.s. many people also mentioned we need to limit our use of meat, i agree.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:12:29.047099Z","iopub.execute_input":"2024-12-09T21:12:29.048585Z","iopub.status.idle":"2024-12-09T21:12:29.152252Z","shell.execute_reply.started":"2024-12-09T21:12:29.048538Z","shell.execute_reply":"2024-12-09T21:12:29.151114Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\n\n# Function to count sentences\ndef count_sentences(text):\n    if pd.isna(text):  # Handle missing values\n        return 0\n    return len(sent_tokenize(text))\n\n# Add num_sentences_body and num_sentences_text columns\ndata['num_sentences_cleaned_body'] = data['cleaned_body'].apply(count_sentences)\ndata['num_sentences_cleaned_text'] = data['cleaned_text'].apply(count_sentences)\n\ndata['num_sentences_body'] = data['body'].apply(count_sentences)\ndata['num_sentences_text'] = data['text'].apply(count_sentences)\n\n# Verify the result\nprint(data[['cleaned_body', 'num_sentences_cleaned_body']].head())\nprint(data[['cleaned_text', 'num_sentences_cleaned_text']].head())\n\n\n# Verify the result for body\nprint(data[['body', 'num_sentences_body']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:12:37.698824Z","iopub.execute_input":"2024-12-09T21:12:37.700043Z","iopub.status.idle":"2024-12-09T21:14:10.587030Z","shell.execute_reply.started":"2024-12-09T21:12:37.699952Z","shell.execute_reply":"2024-12-09T21:14:10.585948Z"}},"outputs":[{"name":"stdout","text":"                                        cleaned_body  \\\n0  this post has been locked due to a large numbe...   \n1  i'll share an anecdote. i'm well read enough t...   \n2  please forgive not having links. i wrote this ...   \n3  youre not quite thinking about this the right ...   \n4  how do you tell the difference between actual ...   \n\n   num_sentences_cleaned_body  \n0                           1  \n1                          15  \n2                          21  \n3                          15  \n4                           1  \n                                        cleaned_text  \\\n0  when you ask the average conservative why they...   \n1  when you ask the average conservative why they...   \n2  when you ask the average conservative why they...   \n3  when you ask the average conservative why they...   \n4  when you ask the average conservative why they...   \n\n   num_sentences_cleaned_text  \n0                          15  \n1                          15  \n2                          15  \n3                          15  \n4                          15  \n                                                body  num_sentences_body\n0  This post has been locked due to a large numbe...                   1\n1  I'll share an anecdote. I'm well read enough t...                  17\n2  Please forgive not having links. I wrote this ...                  14\n3  You’re not quite thinking about this the right...                  15\n4  [https://cei.org/blog/wrong-again-50-years-of-...                   1\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Find the maximum number of sentences\nmax_sentences_cleaned_body = max(data['num_sentences_cleaned_body'])\nmax_sentec_body= max(data['num_sentences_body'])\n\n# Print the maximum number of sentences\nprint(f\"Maximum number of sentences: {max_sentences_cleaned_body}\")\nprint(\"------------------------------\")\nprint(f\"Maximum number of sentences: {max_sentec_body}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:14:10.589362Z","iopub.execute_input":"2024-12-09T21:14:10.589843Z","iopub.status.idle":"2024-12-09T21:14:10.606664Z","shell.execute_reply.started":"2024-12-09T21:14:10.589796Z","shell.execute_reply":"2024-12-09T21:14:10.605479Z"}},"outputs":[{"name":"stdout","text":"Maximum number of sentences: 113\n------------------------------\nMaximum number of sentences: 115\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Get the row(s) where the maximum number of sentences occur\nmax_sentence_rows = data[data['num_sentences_body'] == max_sentec_body]\n\n\n# Print the corresponding comments\nprint(\"Comments with the maximum number of sentences:\")\nprint(max_sentence_rows[['body', 'num_sentences_body']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:14:10.607862Z","iopub.execute_input":"2024-12-09T21:14:10.608220Z","iopub.status.idle":"2024-12-09T21:14:10.618509Z","shell.execute_reply.started":"2024-12-09T21:14:10.608166Z","shell.execute_reply":"2024-12-09T21:14:10.617372Z"}},"outputs":[{"name":"stdout","text":"Comments with the maximum number of sentences:\n                                                    body  num_sentences_body\n13610  > The most conservative analyses of species ex...                 115\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"print(data['cleaned_body'][13610])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:14:10.620532Z","iopub.execute_input":"2024-12-09T21:14:10.620833Z","iopub.status.idle":"2024-12-09T21:14:10.629899Z","shell.execute_reply.started":"2024-12-09T21:14:10.620805Z","shell.execute_reply":"2024-12-09T21:14:10.628788Z"}},"outputs":[{"name":"stdout","text":"the most conservative analyses of species extinction find an extinction rate of 100x background level. some of them are as high as 1000 background level. you can go review the methods for evaluating biocapacity and propose better measures. like all trends, we measure them with data. good luck finding data with positive implications for our ecology. this would be useful if this was historically constant - it isn't. there have been several mass extinction events. not only that, there have been several significant extinction events that didn't rate'mass'status. you are not understanding variability at all. ce ages and geological history occur over tens of thousands of years, not a few decades. the mere rate of increase is evident of a catastrophic event like a comet hitting earth. there is an expected rate of extinctions, that's why our measures compart current extinctions rates to background extinction rates. furthermore, this is just further evidence to my point. you have to compare what damage we've done in just a few years to tens of thousands of years of damage to even stake your claim. there is no reason to believe change is'gradual'. you are making the assumption that there is a steady rate of change and that simply is not true. changes happen slowly and abruptly - for many different reasons. you are arguing for a'static'earth based on recent and recent is really generous, information. if by all the time you mean slowly tens of thousands of years. actually, no. i mean all time scales and at all size scales. habitats change frequently. the stupid simplest example i can give is a hay field/meadow allowed to revert to woodland. takes years to happen - not thousands of years. you have this mistaken concept that there is'one earth'that should be a specific way. that just does not match history, geological history, nor reality. so you are claiming that the notion that resources are finite within an ecosystem is bunk? that's all biocapacity is. it is a measure of how many resources earth has, has quickly natural resources replenish, and how quickly we use them. a positive biocapacity means we use resources slower than earth replenishes them. you are welcome to dispute the laws of physics. the assumptions put into your concept are'bunk'. resources may be finite, but trying to claim a specific'capacity'is nothing but a politically motivated idea for what you think should be the case. i can cite the paper that listed the carrying capacity of the earth at a billion-billion people based on thermodynamics. what you are trying to refer to is'carrying capacity'and there is not a good means to define that for humans since we can readily transport resources across vast distances and we can actively modify an environment to suit us. restoration of habitats. repopulation of species. decarbonization. restoration to what? why is that the'correct'habtitat? seriously. why is a specific choice better than another? decarbonization tells me what i need to know about your'politics'. fun fact - co2 levels are lower now than they have been the planets geologic history. why is it needed to'lower'them? explain to me why exactly this value you want is more correct than a previous level in the planets history? you should be able to do this if you are demanding it. quite the opposite. the problem is that our position in the ecosystem is unbalanced because we have broken free of the state of nature. the planet is the way it is. we either work within the physical limitations of that system or not. we have chosen not to. this is a total line of bull. we are part of nature and completely incapable of'breaking nature'. everything we do is'natural'. good, bad, otherwise, we are part of the planets ecosystems. yes, but we won't. humans have proven to be one of the most adaptable animal yet to exist on the planet. probably the most adaptable if you limit the list to larger creatures. there is little reason to believe we wouldnt survive. no way. it would have been catastrophic for europe and caused immense damage to humanity. plants and then animals would relatively quickly reclaim those places as we saw in the containment zone. where human activity ceases, ecological recovery occurs. i don't think you understand just how dangerous the event was and what could have happended should that reactor have melted down through the containment and exploded when it hit groundwater. as for recovery - if you include dna damage - sure. but if it actually went to the'worst case'possible, it would have been a mass extinction event for much of the world. a. says the one who is disputing the laws of physics. b. you are literally disputing facts. chernobyl ecosystems'remarkably healthy what does it mean to your credibility when you dispute demonstrable facts without evidence? saying something does not make it true. as for remarkably healthy - sure. never mind the dna damage and the fact humans won't be able to live there for tens of thousands of years. that eating anything grown there is injesting poison. yea - i am the one who is denying physics........ people do all the time. parents have children. children care for their parents. people pay taxes. people live and thrive in systems that limit their quality of life. you are just not willing to make sacrifices for the future of your species and all species. you are entitled to take that position, you should just be open about it. no - people don't give up quality of life all the time. having children is seen as improving quality of thier lives. you are nuts if you think people are going to merely accept your demands. you don't even need to take my word for it. see how far the'green'movement has gotten worldwide with its pet projects. if it is a quality if life issue - it doesn't happen. capitalism had caused the greatest loss of species in all of human existence, a rate of loss comparable to when a meteor destroyed the dinosaurs. we literally call this the sixth extinction. the 5th was the cretaceous-tertiary extinction. you cannot attribute that to captitalism when some of the worst ecological offenders are communist countries. although - they did result in something like 100 million dead humans last century so that could be twisted into some warped idea of good thing if you are a eco-zealot. what do you think the quality of life is on a planet with dead oceans, little arable land, and a global center that indefinitely exceeds the wet bulb temperature? i don't care what ideology underlies the drive for sustainability. it's just clear that the need of capitalism for unlimited growth is the cause of the problem. capitalism and sustainability are simply incompatible. a society cannot grow indefinitely when resources are limited. it must achieve a state of balance with its ecosystem. every collapse of a civilization was caused by this simple oversight. ours will inevitably follow if we do not learn from history or science. i've heard it all before. deadlines that come an go without any it every happening. chicken little - the sky is not falling. we are not on the verge of all dying off. if you want the best solution to solving complex large scale problems - it has been capitalism. a system that works on the underlying human nature. that is what will succeed. your desire to fight human nature is a fools errand - proven time and again to fail in the real world. i notice how you never acknowledge this. you don't have a viable option to suggest. you just want to rail against the as of yet - best form of economic system yet developed. do yourself a favor - get a little context and stop believing the zealots who are preaching doom/gloom and the end of the world. you'll be much happier in the long run.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"print(data['body'][13610])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:14:10.631330Z","iopub.execute_input":"2024-12-09T21:14:10.631805Z","iopub.status.idle":"2024-12-09T21:14:10.646476Z","shell.execute_reply.started":"2024-12-09T21:14:10.631759Z","shell.execute_reply":"2024-12-09T21:14:10.645367Z"}},"outputs":[{"name":"stdout","text":"> The most conservative analyses of species extinction find an extinction rate of 100x background level. Some of them are as high as 1000 background level. You can go review the methods for evaluating biocapacity and propose better measures. Like all trends, we measure them with data. Good luck finding data with positive implications for our ecology.\n\nThis would be useful if this was historically constant - it isn't. There have been several mass extinction events. Not only that, there have been several significant extinction events that didn't rate 'mass' status.\n\nYou are not understanding variability at all.\n\n>ce ages and geological history occur over tens of thousands of years, not a few decades. The mere rate of increase is evident of a catastrophic event like a comet hitting Earth. There is an expected rate of extinctions, that's why our measures compart current extinctions rates to background extinction rates. Furthermore, this is just further evidence to my point. You have to compare what damage we've done in just a few years to tens of thousands of years of damage to even stake your claim.\n\nThere is no reason to believe change is 'gradual'. You are making the assumption that there is a steady rate of change and that simply is not true. Changes happen slowly and abruptly - for many different reasons. You are arguing for a 'static' earth based on recent and recent is really generous, information. \n\n>If by \"all the time\" you mean \"slowly tens of thousands of years.\"\n\nActually, no. I mean all time scales and at all size scales. Habitats change frequently. The stupid simplest example I can give is a hay field/meadow allowed to revert to woodland. Takes years to happen - not thousands of years. You have this mistaken concept that there is 'one earth' that should be a specific way. \n\nThat just does not match history, geological history, nor reality.\n\n>So you are claiming that the notion that resources are finite within an ecosystem is bunk? That's all biocapacity is. It is a measure of how many resources Earth has, has quickly natural resources replenish, and how quickly we use them. A positive biocapacity means we use resources slower than Earth replenishes them. You are welcome to dispute the laws of physics.\n\nThe assumptions put into your concept are 'bunk'. Resources may be finite, but trying to claim a specific 'capacity' is nothing but a politically motivated idea for what you think should be the case. I can cite the paper that listed the carrying capacity of the earth at a billion-billion people based on thermodynamics. \n\nWhat you are trying to refer to is 'carrying capacity' and there is not a good means to define that for humans since we can readily transport resources across vast distances and we can actively modify an environment to suit us. \n\n>Restoration of habitats. Repopulation of species. Decarbonization.\n\nRestoration to what? Why is that the 'correct' habtitat? Seriously. Why is a specific choice better than another?\n\nDecarbonization tells me what I need to know about your 'politics'. Fun fact - CO2 levels are lower now than they have been the planets geologic history. Why is it needed to 'lower' them? Explain to me why exactly this value you want is more correct than a previous level in the planets history?\n\nYou should be able to do this if you are demanding it.\n\n>Quite the opposite. The problem is that our position in the ecosystem is unbalanced because we have broken free of the state of nature. The planet is the way it is. We either work within the physical limitations of that system or not. We have chosen not to.\n\nThis is a total line of bull. We are PART of nature and completely incapable of 'breaking nature'. Everything we do is 'natural'. Good, bad, otherwise, we are part of the planets ecosystems.\n\n>Yes, but we won't.\n\nHumans have proven to be one of the most adaptable animal yet to exist on the planet. Probably the most adaptable if you limit the list to larger creatures. There is little reason to believe we wouldnt survive.\n\n>No way. It would have been catastrophic for Europe and caused immense damage to humanity. Plants and then animals would relatively quickly reclaim those places as we saw in the containment zone. Where human activity ceases, ecological recovery occurs.\n\nI don't think you understand just how dangerous the event was and what could have happended should that reactor have melted down through the containment and exploded when it hit groundwater. \n\nAs for recovery - if you include DNA damage - sure. But if it actually went to the 'worst case' possible, it would have been a mass extinction event for much of the world. \n\n>A. Says the one who is disputing the laws of physics.\n>\n>B. You are literally disputing facts. Chernobyl ecosystems 'remarkably healthy What does it mean to your credibility when you dispute demonstrable facts without evidence?\n\nSaying something does not make it true. As for remarkably healthy - sure. Never mind the DNA damage and the fact humans won't be able to live there for tens of thousands of years. That eating anything grown there is injesting poison. \n\nYea - I am the one who is denying physics........\n\n>People do all the time. Parents have children. Children care for their parents. People pay taxes. People live and thrive in systems that limit their quality of life. You are just not willing to make sacrifices for the future of your species and all species. You are entitled to take that position, you should just be open about it.\n\nNo - people don't give up quality of life all the time. Having children is seen as improving quality of thier lives.\n\nYou are nuts if you think people are going to merely accept your demands. You don't even need to take my word for it. See how far the 'Green' movement has gotten worldwide with its pet projects. If it is a quality if life issue - it doesn't happen.\n\n>Capitalism had caused the greatest loss of species in all of human existence, a rate of loss comparable to when a meteor destroyed the dinosaurs. We literally call this The Sixth Extinction. The 5th was the Cretaceous-Tertiary extinction.\n\nYou cannot attribute that to Captitalism when some of the worst ecological offenders are communist countries.\n\nAlthough - they did result in something like 100 million dead humans last century so that could be twisted into some warped idea of good thing if you are a eco-zealot. \n\n>What do you think the quality of life is on a planet with dead oceans, little arable land, and a global center that indefinitely exceeds the wet bulb temperature? I don't care what ideology underlies the drive for sustainability. It's just clear that the need of capitalism for unlimited growth is the cause of the problem. Capitalism and sustainability are simply incompatible. A society cannot grow indefinitely when resources are limited. It must achieve a state of balance with its ecosystem. Every collapse of a civilization was caused by this simple oversight. Ours will inevitably follow if we do not learn from history or science.\n\nI've heard it all before. Deadlines that come an go without any it every happening. Chicken little - the Sky is NOT falling. We are not on the verge of all dying off. If you want the best solution to solving complex large scale problems - it has been capitalism. A system that works on the underlying human nature. That is what will succeed.\n\nYour desire to fight human nature is a fools errand - proven time and again to fail in the real world. \n\nI notice how you never acknowledge this. You don't have a viable option to suggest. You just want to rail against the as of yet - best form of economic system yet developed. \n\nDo yourself a favor - get a little context and stop believing the zealots who are preaching doom/gloom and the end of the world. You'll be much happier in the long run.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# save the dataset \n# Save the DataFrame to a CSV file\noutput_path = \"/kaggle/working/updated_data_new.csv\"  # Specify the path and filename\ndata.to_csv(output_path, index=False)\n\nprint(f\"Dataset saved to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:14:46.747214Z","iopub.execute_input":"2024-12-09T21:14:46.747620Z","iopub.status.idle":"2024-12-09T21:14:57.229257Z","shell.execute_reply.started":"2024-12-09T21:14:46.747586Z","shell.execute_reply":"2024-12-09T21:14:57.228075Z"}},"outputs":[{"name":"stdout","text":"Dataset saved to /kaggle/working/updated_data_new.csv\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Group by 'link_id' (post ID) and count the number of comments\npost_comment_counts = data.groupby('link_id').size().reset_index(name='num_comments')\n\n# Display the result\nprint(\"number of posts\",len(post_comment_counts))\nprint(post_comment_counts.head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:14:57.231327Z","iopub.execute_input":"2024-12-09T21:14:57.231876Z","iopub.status.idle":"2024-12-09T21:14:57.256020Z","shell.execute_reply.started":"2024-12-09T21:14:57.231812Z","shell.execute_reply":"2024-12-09T21:14:57.254993Z"}},"outputs":[{"name":"stdout","text":"number of posts 331\n   link_id  num_comments\n0  10a2an3           201\n1  10cktde            45\n2  10djn5r           163\n3  10i58lq           168\n4  10li7zz           137\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Optionally, save the result to a CSV file\noutput_path = \"/kaggle/working/postcomment_counts_new.csv\"\npost_comment_counts.to_csv(output_path, index=False)\nprint(f\"Post-comment counts saved to {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:15:09.252366Z","iopub.execute_input":"2024-12-09T21:15:09.252775Z","iopub.status.idle":"2024-12-09T21:15:09.261384Z","shell.execute_reply.started":"2024-12-09T21:15:09.252741Z","shell.execute_reply":"2024-12-09T21:15:09.259883Z"}},"outputs":[{"name":"stdout","text":"Post-comment counts saved to /kaggle/working/postcomment_counts_new.csv\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import get_scheduler\nfrom tqdm import tqdm\n\n# Ensure GPU is enabled\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f\"Running on: {device}\")\n\n\n\n# Select a subset of 500 rows for testing\ndf = df.sample(1000).reset_index(drop=True)\n\n# Ensure the dataset contains required columns\nassert 'cleaned_text' in df.columns, \"The dataset must contain a 'post' column.\"\nassert 'cleaned_body' in df.columns, \"The dataset must contain a 'comment' column.\"\n\n# Add a dummy label column if missing\nif 'label' not in df.columns:\n    df['label'] = (torch.rand(len(df)) > 0.5).int()  # Random binary labels for testing\n\n# Split dataset\ntrain_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# 2. Tokenize and create DataLoaders\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef prepare_data(df):\n    input_ids = []\n    attention_masks = []\n    labels = []\n\n    for _, row in df.iterrows():\n        post = row['post']\n        comment = row['comment']\n        label = row['label']\n\n        # Tokenize the post-comment pair\n        inputs = tokenizer(\n            post,\n            comment,\n            add_special_tokens=True,\n            max_length=128,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        input_ids.append(inputs[\"input_ids\"])\n        attention_masks.append(inputs[\"attention_mask\"])\n        labels.append(label)\n\n    # Convert to tensors\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels)\n\n    return TensorDataset(input_ids, attention_masks, labels)\n\n# Prepare datasets\ntrain_dataset = prepare_data(train_df)\nval_dataset = prepare_data(val_df)\ntest_dataset = prepare_data(test_df)\n\n# Create DataLoaders\nbatch_size = 16\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\nval_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\ntest_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n\n# 3. Define the model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nmodel.to(device)\n\n# 4. Define optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\nnum_training_steps = len(train_dataloader) * 4  # Assume 4 epochs\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# 5. Training loop\nepochs = 4\nloss_fn = CrossEntropyLoss()\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n\n    # Training\n    model.train()\n    train_loss = 0\n    for batch in tqdm(train_dataloader):\n        batch_input_ids = batch[0].to(device)\n        batch_attention_masks = batch[1].to(device)\n        batch_labels = batch[2].to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n        loss = outputs.loss\n        train_loss += loss.item()\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n\n    print(f\"Training Loss: {train_loss / len(train_dataloader):.4f}\")\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for batch in val_dataloader:\n            batch_input_ids = batch[0].to(device)\n            batch_attention_masks = batch[1].to(device)\n            batch_labels = batch[2].to(device)\n\n            outputs = model(batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n            val_loss += outputs.loss.item()\n\n            # Accuracy\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)\n            correct += (predictions == batch_labels).sum().item()\n            total += batch_labels.size(0)\n\n    print(f\"Validation Loss: {val_loss / len(val_dataloader):.4f}\")\n    print(f\"Validation Accuracy: {correct / total:.4f}\")\n\n# 6. Save the model\nmodel_save_path = \"/kaggle/working/bert_model.pt\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n\n# 7. Testing\nprint(\"Testing the model...\")\nmodel.eval()\ntest_loss = 0\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        batch_input_ids = batch[0].to(device)\n        batch_attention_masks = batch[1].to(device)\n        batch_labels = batch[2].to(device)\n\n        outputs = model(batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n        test_loss += outputs.loss.item()\n\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n        correct += (predictions == batch_labels).sum().item()\n        total += batch_labels.size(0)\n\nprint(f\"Test Loss: {test_loss / len(test_dataloader):.4f}\")\nprint(f\"Test Accuracy: {correct / total:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T21:15:13.828869Z","iopub.execute_input":"2024-12-09T21:15:13.829902Z","iopub.status.idle":"2024-12-09T21:15:14.749190Z","shell.execute_reply.started":"2024-12-09T21:15:13.829857Z","shell.execute_reply":"2024-12-09T21:15:14.747335Z"}},"outputs":[{"name":"stdout","text":"Running on: cpu\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 1. Load dataset\u001b[39;00m\n\u001b[1;32m     15\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/mydata/merged_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update this\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Select a subset of 500 rows for testing\u001b[39;00m\n\u001b[1;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m500\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/mydata/merged_data.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/mydata/merged_data.csv'","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"\n# Ensure the dataset contains required columns\nassert 'cleaned_text' in df.columns, \"The dataset must contain a 'cleaned_text' column for posts.\"\nassert 'cleaned_body' in df.columns, \"The dataset must contain a 'cleaned_body' column for comments.\"\n\n# Add a dummy label column if missing\nif 'label' not in df.columns:\n    df['label'] = (torch.rand(len(df)) > 0.5).int()  # Random binary labels for testing\n\n# Select a subset of 500 rows for quick testing\ndf = df.sample(500).reset_index(drop=True)\n\n# Split dataset\ntrain_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# 2. Tokenize and create DataLoaders\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef prepare_data(df):\n    input_ids = []\n    attention_masks = []\n    labels = []\n\n    for _, row in df.iterrows():\n        post = row['cleaned_text']\n        comment = row['cleaned_body']\n        label = row['label']\n\n        # Tokenize the post-comment pair\n        inputs = tokenizer(\n            post,\n            comment,\n            add_special_tokens=True,\n            max_length=512,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        input_ids.append(inputs[\"input_ids\"])\n        attention_masks.append(inputs[\"attention_mask\"])\n        labels.append(label)\n\n    # Convert to tensors\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels)\n\n    return TensorDataset(input_ids, attention_masks, labels)\n\n# Prepare datasets\ntrain_dataset = prepare_data(train_df)\nval_dataset = prepare_data(val_df)\ntest_dataset = prepare_data(test_df)\n\n# Create DataLoaders\nbatch_size = 16\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\nval_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\ntest_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n\n# 3. Define the model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nmodel.to(device)\n\n# 4. Define optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\nnum_training_steps = len(train_dataloader) * 4  # Assume 4 epochs\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# 5. Training loop\nepochs = 4\nloss_fn = CrossEntropyLoss()\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n\n    # Training\n    model.train()\n    train_loss = 0\n    for batch in tqdm(train_dataloader):\n        batch_input_ids = batch[0].to(device)\n        batch_attention_masks = batch[1].to(device)\n        batch_labels = batch[2].to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n        loss = outputs.loss\n        train_loss += loss.item()\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n\n    print(f\"Training Loss: {train_loss / len(train_dataloader):.4f}\")\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for batch in val_dataloader:\n            batch_input_ids = batch[0].to(device)\n            batch_attention_masks = batch[1].to(device)\n            batch_labels = batch[2].to(device)\n\n            outputs = model(batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n            val_loss += outputs.loss.item()\n\n            # Accuracy\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)\n            correct += (predictions == batch_labels).sum().item()\n            total += batch_labels.size(0)\n\n    print(f\"Validation Loss: {val_loss / len(val_dataloader):.4f}\")\n    print(f\"Validation Accuracy: {correct / total:.4f}\")\n\n# 6. Save the model\nmodel_save_path = \"/kaggle/working/bert_model.pt\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n\n# 7. Testing\nprint(\"Testing the model...\")\nmodel.eval()\ntest_loss = 0\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        batch_input_ids = batch[0].to(device)\n        batch_attention_masks = batch[1].to(device)\n        batch_labels = batch[2].to(device)\n\n        outputs = model(batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n        test_loss += outputs.loss.item()\n\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n        correct += (predictions == batch_labels).sum().item()\n        total += batch_labels.size(0)\n\nprint(f\"Test Loss: {test_loss / len(test_dataloader):.4f}\")\nprint(f\"Test Accuracy: {correct / total:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}